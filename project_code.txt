[FILE-PATH: constants/configs.py]
[CODE]
CREATE_PATCHES_PRESET = {
  'seg_level': -1,
  'sthresh': 8,
  'mthresh': 7,
  'close': 4,
  'use_otsu': True,
  'a_t': 16,
  'a_h': 4,
  'max_n_holes': 8,
  'vis_level': -1,
  'line_thickness': 100,
  'white_thresh': 5,
  'black_thresh': 50,
  'use_padding': True,
  'contour_fn': 'four_pt',
  'keep_ids': 'none',
  'exclude_ids': 'none'
}

[FILE-PATH: constants/encoders.py]
[CODE]
from enum import Enum

class Encoders(Enum):
  RESNET18 = 'resnet18'     # implemented, tested
  RESNET34 = 'resnet34'     # implemented, tested
  RESNET50 = 'resnet50'     # implemented, tested
  RESNET101 = 'resnet101'   # implemented, tested
  RESNET152 = 'resnet152'   # implemented, tested

[FILE-PATH: constants/mil_models.py]
[CODE]
from enum import Enum

class MILModels(Enum):
  TRANS_MIL = 'trans_mil'

[FILE-PATH: constants/misc.py]
[CODE]
DATASET_BASE_DIRECTORY = '/kaggle/input/prostate-cancer-grade-assessment'
DATASET_SLIDES_FOLDER_NAME = 'train_images'
# DATASET_BASE_DIRECTORY = 'data'
# DATASET_SLIDES_FOLDER_NAME = 'slides'
DATASET_INFO_FILE_NAME = 'train.csv'

OUTPUT_BASE_DIRECTORY = 'output'

[FILE-PATH: constants/__init__.py]
[CODE]


[FILE-PATH: encoders/resnet.py]
[CODE]
import torch.nn as nn
import torch.utils.model_zoo as model_zoo
from torchvision import transforms

from constants.encoders import Encoders

model_urls = {
  Encoders.RESNET18.value: 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
  Encoders.RESNET34.value: 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
  Encoders.RESNET50.value: 'http://download.pytorch.org/models/resnet50-19c8e357.pth',
  Encoders.RESNET101.value: 'http://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
  Encoders.RESNET152.value: 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}

class BasicBlockBaseline(nn.Module):
  expansion = 1

  def __init__(
    self,
    inplanes,
    planes,
    stride = 1,
    downsample = None
  ):
    super(BasicBlockBaseline, self).__init__()
    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
    self.bn1 = nn.BatchNorm2d(planes)
    self.relu = nn.ReLU(inplace=True)
    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)
    self.bn2 = nn.BatchNorm2d(planes)
    self.downsample = downsample
    self.stride = stride

  def forward(self, x):
    residual = x

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)

    if self.downsample is not None:
      residual = self.downsample(x)

    out += residual
    out = self.relu(out)

    return out

class BottleneckBaseline(nn.Module):
  expansion = 4

  def __init__(
    self,
    inplanes,
    planes,
    stride = 1,
    downsample = None
  ):
    super(BottleneckBaseline, self).__init__()
    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
    self.bn1 = nn.BatchNorm2d(planes)
    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
    self.bn2 = nn.BatchNorm2d(planes)
    self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
    self.bn3 = nn.BatchNorm2d(planes * self.expansion)
    self.relu = nn.ReLU(inplace=True)
    self.downsample = downsample
    self.stride = stride

  def forward(self, x):
    residual = x

    out = self.conv1(x)
    out = self.bn1(out)
    out = self.relu(out)

    out = self.conv2(out)
    out = self.bn2(out)
    out = self.relu(out)

    out = self.conv3(out)
    out = self.bn3(out)

    if self.downsample is not None:
      residual = self.downsample(x)

    out += residual
    out = self.relu(out)

    return out

class ResNetBaseline(nn.Module):
  def __init__(self, block, layers):
    self.inplanes = 64
    super(ResNetBaseline, self).__init__()
    
    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    self.layer1 = self._make_layer(block, 64, layers[0])
    self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
    self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
    # self.layer4 = self._make_layer(block, 512, layers[3], stride=2) # FIX: UPDATED_OTHER_THAN_GPFM_TOOLKIT_CODE
    self.avgpool = nn.AdaptiveAvgPool2d(1) 

    for m in self.modules():
      if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
      elif isinstance(m, nn.BatchNorm2d):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)

  def _make_layer(self, block, planes, blocks, stride=1):
    downsample = None
    if stride != 1 or self.inplanes != planes * block.expansion:
      downsample = nn.Sequential(
        nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
        nn.BatchNorm2d(planes * block.expansion),
      )

    layers = []
    layers.append(block(self.inplanes, planes, stride, downsample))
    self.inplanes = planes * block.expansion
    for i in range(1, blocks):
      layers.append(block(self.inplanes, planes))

    return nn.Sequential(*layers)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    # x = self.layer4(x) # FIX: UPDATED_OTHER_THAN_GPFM_TOOLKIT_CODE

    x = self.avgpool(x)
    x = x.view(x.size(0), -1)

    return x

def get_encoder_baseline(encoder_name, pretrained = False):
  encoder = None

  if encoder_name == Encoders.RESNET18.value: 
    encoder = ResNetBaseline(BasicBlockBaseline, [2, 2, 2, 2])
  elif encoder_name == Encoders.RESNET34.value:
    encoder = ResNetBaseline(BasicBlockBaseline, [3, 4, 6, 3])
  elif encoder_name == Encoders.RESNET50.value:
    encoder = ResNetBaseline(BottleneckBaseline, [3, 4, 6, 3])
  elif encoder_name == Encoders.RESNET101.value:
    encoder = ResNetBaseline(BottleneckBaseline, [3, 4, 23, 3])
  elif encoder_name == Encoders.RESNET152.value:
    encoder = ResNetBaseline(BottleneckBaseline, [3, 8, 36, 3])

  if pretrained:
    encoder = load_pretrained_weights(encoder, encoder_name)

  return encoder

def load_pretrained_weights(encoder, name):
  pretrained_dict = model_zoo.load_url(model_urls[name])
  encoder.load_state_dict(pretrained_dict, strict=False)
  return encoder

def custom_transformer():
  mean = (0.485, 0.456, 0.406)
  std = (0.229, 0.224, 0.225)

  x = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean = mean, std = std)
  ])

  return x

[FILE-PATH: encoders/__init__.py]
[CODE]
import torch
import torchvision

from constants.encoders import Encoders

def get_encoder(
  encoder_name: str,
  device: torch.device,
  gpu_num: int
) -> torch.nn.Module:
  encoder = None
  
  if encoder_name in [
    Encoders.RESNET18.value,
    Encoders.RESNET34.value,
    Encoders.RESNET50.value,
    Encoders.RESNET101.value,
    Encoders.RESNET152.value,
  ]:
    from encoders.resnet import get_encoder_baseline
    encoder = get_encoder_baseline(encoder_name, pretrained=True).to(device)
  else:
    raise NotImplementedError(f'Encoder \'{encoder_name}\' is not implemented, yet!')

  if encoder_name in [
    Encoders.RESNET18.value,
    Encoders.RESNET34.value,
    Encoders.RESNET50.value,
    Encoders.RESNET101.value,
    Encoders.RESNET152.value,
  ]:
    if gpu_num > 1: encoder = torch.nn.parallel.DataParallel(encoder)
    encoder = encoder.eval()

  return encoder

def get_custom_transformer(encoder_name: str) -> torchvision.transforms:
  transformer = None
  
  if encoder_name in [
    Encoders.RESNET18.value,
    Encoders.RESNET34.value,
    Encoders.RESNET50.value,
    Encoders.RESNET101.value,
    Encoders.RESNET152.value,
  ]:
    from encoders.resnet import custom_transformer
    transformer = custom_transformer()
  else:
    raise NotImplementedError(f'Transformer for \'{encoder_name}\' is not implemented, yet!')

  return transformer

[FILE-PATH: mil_models/base_mil.py]
[CODE]
import torch
import torch.nn.functional as F
from torch import nn

class BaseMILModel(nn.Module):
  def __init__(self) -> None:
    super().__init__()
    self.task_adapter = task_logits_adapter

def task_logits_adapter(logits):
  Y_prob = F.softmax(logits, dim = 1)
  Y_hat = torch.topk(logits, 1, dim = 1)[1]
  return logits, Y_prob, Y_hat

[FILE-PATH: mil_models/__init__.py]
[CODE]
import torch

from constants.mil_models import MILModels

def find_mil_model(
  model_name: str,
  in_dim: int,
  n_classes: int,
  drop_out: float,
):
  if model_name.lower() == MILModels.TRANS_MIL.value.lower():
    from .trans_mil import TransMIL
    model = TransMIL(in_dim, n_classes, drop_out = drop_out, activation = 'relu')
    return model
  else:
    raise NotImplementedError(f'MIL model {model_name} not implemented!')

[FILE-PATH: mil_models/trans_mil/__init__.py]
[CODE]
import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from math import ceil
from einops import rearrange, reduce
from torch import nn, einsum

from mil_models.base_mil import BaseMILModel

# helper functions
def exists(val):
  return val is not None

def moore_penrose_iter_pinv(x, iters = 6):
  device = x.device

  abs_x = torch.abs(x)
  col = abs_x.sum(dim = -1)
  row = abs_x.sum(dim = -2)
  z = rearrange(x, '... i j -> ... j i') / (torch.max(col) * torch.max(row))

  I = torch.eye(x.shape[-1], device = device)
  I = rearrange(I, 'i j -> () i j')

  for _ in range(iters):
    xz = x @ z
    z = 0.25 * z @ (13 * I - (xz @ (15 * I - (xz @ (7 * I - xz)))))

  return z

# main attention class
class NystromAttention(nn.Module):
  def __init__(
    self,
    dim,
    dim_head = 64,
    heads = 8,
    num_landmarks = 256,
    pinv_iterations = 6,
    residual = True,
    residual_conv_kernel = 33,
    eps = 1e-8,
    dropout = 0.
  ):
    super().__init__()
    self.eps = eps
    inner_dim = heads * dim_head

    self.num_landmarks = num_landmarks
    self.pinv_iterations = pinv_iterations

    self.heads = heads
    self.scale = dim_head ** -0.5
    self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

    self.to_out = nn.Sequential(
      nn.Linear(inner_dim, dim),
      nn.Dropout(dropout)
    )

    self.residual = residual
    if residual:
      kernel_size = residual_conv_kernel
      padding = residual_conv_kernel // 2
      self.res_conv = nn.Conv2d(heads, heads, (kernel_size, 1), padding = (padding, 0), groups = heads, bias = False)

  def forward(self, x, mask = None, return_attn = False):
    b, n, _, h, m, iters, eps = *x.shape, self.heads, self.num_landmarks, self.pinv_iterations, self.eps

    # pad so that sequence can be evenly divided into m landmarks
    remainder = n % m
    if remainder > 0:
      padding = m - (n % m)
      x = F.pad(x, (0, 0, padding, 0), value = 0)

      if exists(mask):
        mask = F.pad(mask, (padding, 0), value = False)

    # derive query, keys, values
    q, k, v = self.to_qkv(x).chunk(3, dim = -1)
    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))

    # set masked positions to 0 in queries, keys, values
    if exists(mask):
      mask = rearrange(mask, 'b n -> b () n')
      q, k, v = map(lambda t: t * mask[..., None], (q, k, v))

    q = q * self.scale

    # generate landmarks by sum reduction, and then calculate mean using the mask
    l = ceil(n / m)
    landmark_einops_eq = '... (n l) d -> ... n d'
    q_landmarks = reduce(q, landmark_einops_eq, 'sum', l = l)
    k_landmarks = reduce(k, landmark_einops_eq, 'sum', l = l)

    # calculate landmark mask, and also get sum of non-masked elements in preparation for masked mean
    divisor = l
    if exists(mask):
      mask_landmarks_sum = reduce(mask, '... (n l) -> ... n', 'sum', l = l)
      divisor = mask_landmarks_sum[..., None] + eps
      mask_landmarks = mask_landmarks_sum > 0

    # masked mean (if mask exists)
    q_landmarks /= divisor
    k_landmarks /= divisor

    # similarities
    einops_eq = '... i d, ... j d -> ... i j'
    sim1 = einsum(einops_eq, q, k_landmarks)
    sim2 = einsum(einops_eq, q_landmarks, k_landmarks)
    sim3 = einsum(einops_eq, q_landmarks, k)

    # masking
    if exists(mask):
      mask_value = -torch.finfo(q.dtype).max
      sim1.masked_fill_(~(mask[..., None] * mask_landmarks[..., None, :]), mask_value)
      sim2.masked_fill_(~(mask_landmarks[..., None] * mask_landmarks[..., None, :]), mask_value)
      sim3.masked_fill_(~(mask_landmarks[..., None] * mask[..., None, :]), mask_value)

    # eq (15) in the paper and aggregate values
    attn1, attn2, attn3 = map(lambda t: t.softmax(dim = -1), (sim1, sim2, sim3))
    attn2_inv = moore_penrose_iter_pinv(attn2, iters)

    out = (attn1 @ attn2_inv) @ (attn3 @ v)

    # add depth-wise conv residual of values
    if self.residual:
      out += self.res_conv(v)

    # merge and combine heads
    out = rearrange(out, 'b h n d -> b n (h d)', h = h)
    out = self.to_out(out)
    out = out[:, -n:]

    if return_attn:
      attn = attn1 @ attn2_inv @ attn3
      return out, attn

    return out

# transformer

class PreNorm(nn.Module):
  def __init__(self, dim, fn):
    super().__init__()
    self.norm = nn.LayerNorm(dim)
    self.fn = fn

  def forward(self, x, **kwargs):
    x = self.norm(x)
    return self.fn(x, **kwargs)


class FeedForward(nn.Module):
  def __init__(self, dim, mult = 4, dropout = 0.):
    super().__init__()
    self.net = nn.Sequential(
      nn.Linear(dim, dim * mult),
      nn.GELU(),
      nn.Dropout(dropout),
      nn.Linear(dim * mult, dim)
    )

  def forward(self, x):
    return self.net(x)


class Nystromformer(nn.Module):
  def __init__(
    self,
    *,
    dim,
    depth,
    dim_head = 64,
    heads = 8,
    num_landmarks = 256,
    pinv_iterations = 6,
    attn_values_residual = True,
    attn_values_residual_conv_kernel = 33,
    attn_dropout = 0.,
    ff_dropout = 0.   
  ):
    super().__init__()

    self.layers = nn.ModuleList([])
    for _ in range(depth):
      self.layers.append(nn.ModuleList([
        PreNorm(dim, NystromAttention(dim = dim, dim_head = dim_head, heads = heads, num_landmarks = num_landmarks, pinv_iterations = pinv_iterations, residual = attn_values_residual, residual_conv_kernel = attn_values_residual_conv_kernel, dropout = attn_dropout)),
        PreNorm(dim, FeedForward(dim = dim, dropout = ff_dropout))
      ]))

  def forward(self, x, mask = None):
    for attn, ff in self.layers:
      x = attn(x, mask = mask) + x
      x = ff(x) + x
    return x


def initialize_weights(module):
  for m in module.modules():
    if isinstance(m, nn.Linear):
      nn.init.xavier_normal_(m.weight)
      if m.bias is not None:
        m.bias.data.zero_()
    if isinstance(m, nn.LayerNorm):
      nn.init.constant_(m.bias, 0)
      nn.init.constant_(m.weight, 1.0)


class TransLayer(nn.Module):
  def __init__(self, norm_layer=nn.LayerNorm, dim=512):
    super().__init__()
    self.norm = norm_layer(dim)
    self.attn = NystromAttention(
      dim=dim,
      dim_head=dim // 8,
      heads=8,
      num_landmarks=dim // 2,  # number of landmarks
      pinv_iterations=6,  # number of moore-penrose iterations for approximating pinverse. 6 was recommended by the paper
      residual=True,  # whether to do an extra residual with the value or not. supposedly faster convergence if turned on
      dropout=0.1,
    )

  def forward(self, x):
    x = x + self.attn(self.norm(x))
    return x


class PPEG(nn.Module):
  def __init__(self, dim=512):
    super(PPEG, self).__init__()
    self.proj = nn.Conv2d(dim, dim, 7, 1, 7 // 2, groups=dim)
    self.proj1 = nn.Conv2d(dim, dim, 5, 1, 5 // 2, groups=dim)
    self.proj2 = nn.Conv2d(dim, dim, 3, 1, 3 // 2, groups=dim)

  def forward(self, x, H, W):
    B, _, C = x.shape
    cls_token, feat_token = x[:, 0], x[:, 1:]
    cnn_feat = feat_token.transpose(1, 2).view(B, C, H, W)
    x = self.proj(cnn_feat) + cnn_feat + self.proj1(cnn_feat) + self.proj2(cnn_feat)
    x = x.flatten(2).transpose(1, 2)
    x = torch.cat((cls_token.unsqueeze(1), x), dim=1)
    return x


class TransMIL(BaseMILModel):
  def __init__(self, in_dim, n_classes, drop_out, activation):
    super(TransMIL, self).__init__()
    
    self._fc1 = [nn.Linear(in_dim, 512)]
    if activation.lower() == 'relu':
      self._fc1 += [nn.ReLU()]
    elif activation.lower() == 'gelu':
      self._fc1 += [nn.GELU()]
    if drop_out:
      self._fc1 += [nn.Dropout(0.25)]
    self._fc1 = nn.Sequential(*self._fc1)
    
    self.pos_layer = PPEG(dim=512)

    self.cls_token = nn.Parameter(torch.randn(1, 1, 512))
    nn.init.normal_(self.cls_token, std=1e-6)
    self.n_classes = n_classes
    self.layer1 = TransLayer(dim=512)
    self.layer2 = TransLayer(dim=512)
    self.norm = nn.LayerNorm(512)
    self.classifier = nn.Linear(512, self.n_classes)

    self.apply(initialize_weights)

  def forward(self, x, **kwargs):
    if len(x.shape) == 2:
      x = x.expand(1, -1, -1)

    h = x.float()  # [B, n, 1024]

    h = self._fc1(h)  # [B, n, 256]

    # ---->pad
    H = h.shape[1]
    _H, _W = int(np.ceil(np.sqrt(H))), int(np.ceil(np.sqrt(H)))
    add_length = _H * _W - H
    h = torch.cat([h, h[:, :add_length, :]], dim=1)  # [B, N, 256]

    # ---->cls_token
    cls_tokens = self.cls_token.expand(1, -1, -1).cuda()
    h = torch.cat((cls_tokens, h), dim=1)

    # ---->Translayer x1
    h = self.layer1(h)  # [B, N, 256]

    # ---->PPEG
    h = self.pos_layer(h, _H, _W)  # [B, N, 256]

    # ---->Translayer x2
    h = self.layer2(h)  # [B, N, 256]

    # ---->cls_token
    h = self.norm(h)[:, 0]

    # ---->predict
    logits = self.classifier(h)  # [B, n_classes]
    
    wsi_logits, wsi_prob, wsi_label = self.task_adapter(logits)
    outputs = {
      'wsi_logits': wsi_logits,
      'wsi_prob': wsi_prob,
      'wsi_label': wsi_label,
    }
    return outputs

  def relocate(self):
    device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
    self._fc1 = self._fc1.to(device)
    self.pos_layer = self.pos_layer.to(device)
    self.layer1 = self.layer1.to(device)
    self.layer2 = self.layer2.to(device)
    self.norm = self.norm.to(device)
    self.classifier = self.classifier.to(device)

[FILE-PATH: scripts/create_patches.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os 
import sys
import time
import pandas as pd
import numpy as np
import argparse
from tqdm import tqdm

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import DATASET_BASE_DIRECTORY, DATASET_SLIDES_FOLDER_NAME, OUTPUT_BASE_DIRECTORY
from constants.configs import CREATE_PATCHES_PRESET
from utils.helper import create_directories
from utils.wsi_core.batch_process_utils import initialize_dataframe
from utils.wsi_core.whole_slide_image import WholeSlideImage
from utils.wsi_core.wsi_utils import stitch_coords
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'slides_format': 'tiff',
  'patch_level': 0,
  'patch_size': 512,
  'stride_size': 512,
  'skip_existing': True,
  'do_save_masks': False,
  'do_stitching': False,
  'selective_slides': False,
  'selective_slides_csv': os.path.join(os.path.dirname(__file__), '..', 'data_splits', 'train_0.csv'),
  'directories': {
    'slides_directory': os.path.join(DATASET_BASE_DIRECTORY, DATASET_SLIDES_FOLDER_NAME),
    'save_base_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches'),
    'patches_save_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'masks_save_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'masks'),
    'stitches_save_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'stitches'),
  },
  'verbose': False,
}

def transform_presets(preset: dict) -> tuple:
  seg_params = {
    'seg_level': int(preset['seg_level']),
    'sthresh': int(preset['sthresh']),
    'mthresh': int(preset['mthresh']),
    'close': int(preset['close']),
    'use_otsu': bool(preset['use_otsu']),
    'keep_ids': preset['keep_ids'],
    'exclude_ids': preset['exclude_ids'],
  }

  filter_params = {
    'a_t': int(preset['a_t']),
    'a_h': int(preset['a_h']),
    'max_n_holes': int(preset['max_n_holes']),
  }

  vis_params = {
    'vis_level': int(preset['vis_level']),
    'line_thickness': int(preset['line_thickness']),
  }

  patch_params = {
    'use_padding': bool(preset['use_padding']),
    'contour_fn': preset['contour_fn'],
  }

  return seg_params, filter_params, vis_params, patch_params

def stitching(
  file_path,
  wsi_object: WholeSlideImage,
  downscale = 64,
  verbose = False
):
	start = time.time()

	heatmap = stitch_coords(
    file_path,
    wsi_object,
    downscale = downscale,
    bg_color = (0, 0, 0),
    alpha = -1,
    draw_grid = False,
    verbose = verbose
  )

	total_time = time.time() - start	
	return heatmap, total_time

def segment(
  wsi_object: WholeSlideImage,
  seg_params = None,
  filter_params = None,
  mask_file = None
):
	start_time = time.time()

	if mask_file is not None: 
		wsi_object.init_segmentation(mask_file)
	else:
		wsi_object.segment_tissue(**seg_params, filter_params = filter_params)

	seg_time_elapsed = time.time() - start_time
	return wsi_object, seg_time_elapsed

def patching(
  wsi_object: WholeSlideImage,
  **kwargs
):
	start_time = time.time()
	file_path = wsi_object.process_contours(**kwargs)
	patch_time_elapsed = time.time() - start_time
	return file_path, patch_time_elapsed

def segment_and_patch(
  slides_directory: str,
  save_base_directory: str,
  patches_save_directory: str,
  masks_save_directory: str,
  stitches_save_directory: str,
  patch_size: int = 512,
  step_size: int = 512,
  preset: dict = {
    'seg_level': -1,
    'sthresh': 8,
    'mthresh': 7, 
    'close': 4,
    'use_otsu': False,
    'keep_ids': 'none', 
    'exclude_ids': 'none',
    'a_t': 100,
    'a_h': 16,
    'max_n_holes': 8,
    'vis_level': -1,
    'line_thickness': 500,
    'use_padding': True,
    'contour_fn': 'four_pt',
  },
  patch_level: int = 0,
  slides_format: str = 'tiff',
  skip_existing: bool = True,
  do_segmentation: bool = True,
  do_save_masks: bool = True,
  do_patch: bool = True,
  do_stitching: bool = True,
  verbose: bool = False,
  selective_slides: bool = False,
  selective_slides_csv: str = None
) -> dict:
  if selective_slides and selective_slides_csv is not None:
    selective_slides_df = pd.read_csv(selective_slides_csv)
    selective_slides_ids = selective_slides_df['image_id'].tolist()
  
  slides = []
  for root, dirs, filenames in os.walk(slides_directory):
    for filename in filenames:
      if filename.endswith(slides_format):
        if selective_slides:
          slide_id = os.path.basename(filename).split('.')[0]
          if slide_id not in selective_slides_ids: continue
        slides.append(os.path.join(root, filename))

  seg_params, filter_params, vis_params, patch_params = transform_presets(preset)

  df = initialize_dataframe(
    slides = slides,
    seg_params = seg_params,
    filter_params = filter_params,
    vis_params = vis_params,
    patch_params = patch_params,
  )

  process_stack = df[df['process'] == 1]
  total_slides = len(process_stack)

  times = {
    'segmentation': 0,
    'patching': 0,
    'stitching': 0,
  }
  
  counts = {
    'segmentation': 0,
    'patching': 0,
    'stitching': 0,
  }
  
  start_time = time.time()

  process_list_autogen_path = os.path.join(save_base_directory, 'process_list_autogen.csv')
  for i in tqdm(range(total_slides), desc="Creating Patches", unit="slide"):
    df.to_csv(process_list_autogen_path, index=False)

    index = process_stack.index[i]
    slide_id = os.path.basename(process_stack.loc[index, 'slide_id']).split('.')[0]
    if verbose: logger.info(f'Processing Slide ID: {slide_id} ({i + 1}/{total_slides})')

    df.loc[index, 'process'] = 0

    if skip_existing and os.path.isfile(os.path.join(patches_save_directory, slide_id + '.h5')):
      if verbose:  logger.warning(f'(SKIPPED) {slide_id} already exists in destination location!')
      df.loc[index, 'status'] = 'already_exist'
      continue

    full_slide_path = os.path.join(slides_directory, slide_id + '.' + slides_format)
    try:
      wsi_object = WholeSlideImage(full_slide_path, verbose)
    except:
      if verbose: logger.error(f'{slide_id} failed to load!')
      continue

    current_vis_params = {}
    for key in vis_params.keys():
      current_vis_params.update({ key: df.loc[index, key] })
    
    current_filter_params = {}
    for key in filter_params.keys():
      current_filter_params.update({ key: df.loc[index, key] })
    
    current_seg_params = {}
    for key in seg_params.keys():
      current_seg_params.update({ key: df.loc[index, key] })
      
    current_patch_params = {}
    for key in patch_params.keys():
      current_patch_params.update({ key: df.loc[index, key] })

    if current_vis_params['vis_level'] < 0:
      if len(wsi_object.level_dim) == 1:
        current_vis_params['vis_level'] = 0
      else:
        wsi = wsi_object.get_open_slide()
        best_level = wsi.get_best_level_for_downsample(64)
        current_vis_params['vis_level'] = best_level

    if current_seg_params['seg_level'] < 0:
      if len(wsi_object.level_dim) == 1:
        current_seg_params['seg_level'] = 0
      else:
        wsi = wsi_object.get_open_slide()
        best_level = wsi.get_best_level_for_downsample(64)
        current_seg_params['seg_level'] = best_level

    keep_ids = str(current_seg_params['keep_ids'])
    if keep_ids != 'none' and len(keep_ids) > 0:
      str_ids = current_seg_params['keep_ids']
      current_seg_params['keep_ids'] = np.array(str_ids.split(',')).astype(int)
    else: current_seg_params['keep_ids'] = []

    exclude_ids = str(current_seg_params['exclude_ids'])
    if exclude_ids != 'none' and len(exclude_ids) > 0:
      str_ids = current_seg_params['exclude_ids']
      current_seg_params['exclude_ids'] = np.array(str_ids.split(',')).astype(int)
    else: current_seg_params['exclude_ids'] = []

    w, h = wsi_object.level_dim[current_seg_params['seg_level']]
    if w * h > 1e10: # 10 billion pixels
      if verbose: logger.error('level_dim {} x {} is likely too large for successful segmentation, aborting'.format(w, h))
      df.loc[index, 'status'] = 'failed_seg'
      continue

    df.loc[index, 'vis_level'] = current_vis_params['vis_level']
    df.loc[index, 'seg_level'] = current_seg_params['seg_level']

    current_seg_params['ref_patch_size'] = patch_size

    seg_time_elapsed = 0
    if do_segmentation:
      try:
        wsi_object, seg_time_elapsed = segment(wsi_object, current_seg_params, current_filter_params)
        seg_time_elapsed = max(0, seg_time_elapsed)
        counts['segmentation'] += 1
      except Exception as e:
        if verbose: logger.error(str(e))
        if verbose: logger.error('OpenSlideError, skipped')
        df.loc[index, 'status'] = 'failed_seg'
        continue

    if do_save_masks:
      mask = wsi_object.vis_wsi(**current_vis_params)
      mask_path = os.path.join(masks_save_directory, slide_id + '.jpg')
      if process_stack.loc[index, 'slide_id'].split('.')[-1] in ['jpg']:
        mask = mask.resize([i // 8 for i in mask.size])
      mask.save(mask_path)

    patch_time_elapsed = 0
    if do_patch:
      current_patch_params.update({
        'patch_level': patch_level,
        'patch_size': patch_size,
        'step_size': step_size,
        'save_path': patches_save_directory,
      })
      file_path, patch_time_elapsed = patching(wsi_object, **current_patch_params)
      patch_time_elapsed = max(0, patch_time_elapsed)
      counts['patching'] += 1

    stitch_time_elapsed = 0
    if do_stitching:
      file_path = os.path.join(patches_save_directory, slide_id + '.h5')
      if os.path.isfile(file_path):
        heatmap, stitch_time_elapsed = stitching(file_path, wsi_object, downscale=64, verbose=verbose)
        stitch_time_elapsed = max(0, stitch_time_elapsed)
        counts['stitching'] += 1
        stitch_path = os.path.join(stitches_save_directory, slide_id + '.jpg')
        if process_stack.loc[index, 'slide_id'].split('.')[-1] in ['jpg']:
          heatmap = heatmap.resize([i // 8 for i in heatmap.size])
        heatmap.save(stitch_path)

    if verbose: logger.info(f'\tSegmentation: {seg_time_elapsed:.2f}s')
    if verbose: logger.info(f'\tPatching: {patch_time_elapsed:.2f}s')
    if verbose: logger.info(f'\tStitching: {stitch_time_elapsed:.2f}s')
    df.loc[index, 'status'] = 'processed'

    times['segmentation'] += seg_time_elapsed
    times['patching'] += patch_time_elapsed
    times['stitching'] += stitch_time_elapsed

  end_time = time.time()
  total_time = end_time - start_time

  avg_seg_time = times['segmentation'] / counts['segmentation'] if counts['segmentation'] > 0 else 0
  avg_patch_time = times['patching'] / counts['patching'] if counts['patching'] > 0 else 0
  avg_stitch_time = times['stitching'] / counts['stitching'] if counts['stitching'] > 0 else 0

  df.to_csv(process_list_autogen_path, index=False)
  
  logger.empty_line()
  logger.info("Total Slides Processed: {}", total_slides)
  if counts['segmentation'] > 0:
    logger.info("Segmentation Average Time: {:.2f} seconds ({} slides)", avg_seg_time, counts["segmentation"])
  if counts['patching'] > 0:
    logger.info("Patching Average Time: {:.2f} seconds ({} slides)", avg_patch_time, counts["patching"])
  if counts['stitching'] > 0:
    logger.info("Stitching Average Time: {:.2f} seconds ({} slides)", avg_stitch_time, counts["stitching"])
  logger.info("Total Processing Time: {:.2f} seconds ({:.2f} minutes)", total_time, total_time/60)
  logger.empty_line()
  logger.success("Created Patches Successfully!")

  return {
    'segmentation': avg_seg_time,
    'patching': avg_patch_time, 
    'stitching': avg_stitch_time
  }

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations;")
  logger.text(f"> Slides Format: {CONFIG['slides_format']}")
  logger.text(f"> Patch Level: {CONFIG['patch_level']}")
  logger.text(f"> Patch Size: {CONFIG['patch_size']}")
  logger.text(f"> Stride Size: {CONFIG['stride_size']}")
  logger.text(f"> Skip Existing: {CONFIG['skip_existing']}")
  logger.text(f"> Save Masks: {CONFIG['do_save_masks']}")
  logger.text(f"> Perform Stitching: {CONFIG['do_stitching']}")
  if CONFIG['selective_slides']:
    logger.text(f"> Use Selective Slides: {CONFIG['selective_slides']}")
    logger.text(f"> Selective Slides CSV: {CONFIG['selective_slides_csv']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description = "Create Patches From Slides")
  parser.add_argument(
    "--slides-format",
    type = str,
    default = CONFIG['slides_format'],
    help = f"Slides Format (default: {CONFIG['slides_format']})"
  )
  parser.add_argument(
    "--patch-level",
    type = int,
    default = CONFIG['patch_level'],
    help = f"Patch Level (default: {CONFIG['patch_level']})"
  )
  parser.add_argument(
    "--patch-size",
    type = int,
    default = CONFIG['patch_size'],
    help = f"Patch Size (default: {CONFIG['patch_size']})"
  )
  parser.add_argument(
    "--stride-size",
    type = int,
    default = CONFIG['stride_size'],
    help = f"Stride Size (default: {CONFIG['stride_size']})"
  )
  parser.add_argument(
    "--skip-existing",
    type = bool,
    default = CONFIG['skip_existing'],
    help = f"Skip Existing (default: {CONFIG['skip_existing']})"
  )
  parser.add_argument(
    "--save-masks",
    type = bool,
    default = CONFIG['do_save_masks'],
    help = f"Save Masks (default: {CONFIG['do_save_masks']})"
  )
  parser.add_argument(
    "--stitching",
    type = bool,
    default = CONFIG['do_stitching'],
    help = f"Perform Stitching (default: {CONFIG['do_stitching']})"
  )
  parser.add_argument(
    "--selective-slides",
    type = bool,
    default = CONFIG['selective_slides'],
    help = f"Use Selective Slides (default: {CONFIG['selective_slides']})"
  )
  parser.add_argument(
    "--selective-slides-csv",
    type = str,
    default = CONFIG['selective_slides_csv'],
    help = f"Selective Slides CSV Path (default: {CONFIG['selective_slides_csv']})"
  )
  parser.add_argument(
    "--verbose",
    type = bool,
    default = CONFIG['verbose'],
    help = f"Verbose (default: {CONFIG['verbose']})"
  )
  parser.add_argument(
    "--dataset-base-directory",
    type = str,
    default = DATASET_BASE_DIRECTORY,
    help = f"Dataset Base Directory (default: {DATASET_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--dataset-slides-folder-name",
    type = str,
    default = DATASET_SLIDES_FOLDER_NAME,
    help = f"Dataset Slides Folder Name (default: {DATASET_SLIDES_FOLDER_NAME})"
  )
  parser.add_argument(
    "--output-base-directory",
    type = str,
    default = OUTPUT_BASE_DIRECTORY,
    help = f"Output Base Directory (default: {OUTPUT_BASE_DIRECTORY})"
  )

  args = parser.parse_args()
  
  CONFIG['slides_format'] = args.slides_format
  CONFIG['patch_level'] = args.patch_level
  CONFIG['patch_size'] = args.patch_size
  CONFIG['stride_size'] = args.stride_size
  CONFIG['skip_existing'] = args.skip_existing
  CONFIG['do_save_masks'] = args.save_masks
  CONFIG['do_stitching'] = args.stitching
  CONFIG['selective_slides'] = args.selective_slides
  CONFIG['selective_slides_csv'] = args.selective_slides_csv
  CONFIG['verbose'] = args.verbose
  
  dataset_base_dir = args.dataset_base_directory
  dataset_slides_folder = args.dataset_slides_folder_name
  output_base_dir = args.output_base_directory
  
  CONFIG['directories']['slides_directory'] = os.path.join(dataset_base_dir, dataset_slides_folder)
  CONFIG['directories']['save_base_directory'] = os.path.join(output_base_dir, 'create_patches')
  CONFIG['directories']['patches_save_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['masks_save_directory'] = os.path.join(output_base_dir, 'create_patches', 'masks')
  CONFIG['directories']['stitches_save_directory'] = os.path.join(output_base_dir, 'create_patches', 'stitches')

def main():
  logger.draw_header("Create Patches From Slides")

  load_arguments()
  
  logger.info("Creating Directories...")
  create_directories(CONFIG['directories'])
  
  show_configs()
  
  logger.info("Starting Patch Creation Process...")
  logger.empty_line()
  
  segment_and_patch(
    slides_directory = CONFIG['directories']['slides_directory'],
    save_base_directory = CONFIG['directories']['save_base_directory'],
    patches_save_directory = CONFIG['directories']['patches_save_directory'],
    masks_save_directory = CONFIG['directories']['masks_save_directory'],
    stitches_save_directory = CONFIG['directories']['stitches_save_directory'],
    patch_size = CONFIG['patch_size'],
    step_size = CONFIG['stride_size'],
    preset = CREATE_PATCHES_PRESET,
    patch_level = CONFIG['patch_level'],
    slides_format = CONFIG['slides_format'],
    skip_existing = CONFIG['skip_existing'],
    verbose = CONFIG['verbose'],
    do_save_masks=CONFIG['do_save_masks'],
    do_stitching=CONFIG['do_stitching'],
    selective_slides=CONFIG['selective_slides'],
    selective_slides_csv=CONFIG['selective_slides_csv']
  )
  
  logger.success("Patch Creation Completed Successfully!")

if __name__ == '__main__':
  main()

[FILE-PATH: scripts/create_splits.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os 
import sys
import pandas as pd
import numpy as np
import time
import argparse

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import OUTPUT_BASE_DIRECTORY, DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME
from utils.helper import create_directories
from utils.data_loader import GenericMILDataset
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'validation_fraction': 0.1,
  'test_fraction': 0.1,
  'k_fold': 10,
  'dataset_info_csv': os.path.join(DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME),
  'directories': {
    'save_base_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_splits'),
    'create_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'extract_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_patches'),
    'features_pt_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features', 'pt_files'),
  },
  'verbose': False,
}

def save_splits(
  dataset_splits,
  column_keys,
  filename,
  boolean_style = False
):
  splits = [dataset_splits[i].slide_data['slide_id'] for i in range(len(dataset_splits))]
  if not boolean_style:
    df = pd.concat(splits, ignore_index = True, axis = 1)
    df.columns = column_keys
  else:
    df = pd.concat(splits, ignore_index = True, axis = 0)
    one_hot = np.eye(len(dataset_splits)).astype(bool)
    bool_array = np.repeat(one_hot, [len(dset) for dset in dataset_splits], axis = 0)
    df = pd.DataFrame(
      bool_array,
      index = df.values.tolist(), 
      columns = ['train', 'val', 'test']
    )

  df.to_csv(filename)

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations;")
  logger.text(f"> Validation Fraction: {CONFIG['validation_fraction']}")
  logger.text(f"> Test Fraction: {CONFIG['test_fraction']}")
  logger.text(f"> K-Fold: {CONFIG['k_fold']}")
  logger.text(f"> Dataset Info CSV: {CONFIG['dataset_info_csv']}")
  logger.text(f"> Verbose: {CONFIG['verbose']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description = "Create Dataset Splits")
  parser.add_argument(
    "--validation-fraction",
    type = float,
    default = CONFIG['validation_fraction'],
    help = f"Validation Fraction (default: {CONFIG['validation_fraction']})"
  )
  parser.add_argument(
    "--test-fraction",
    type = float,
    default = CONFIG['test_fraction'],
    help = f"Test Fraction (default: {CONFIG['test_fraction']})"
  )
  parser.add_argument(
    "--k-fold",
    type = int,
    default = CONFIG['k_fold'],
    help = f"K-Fold (default: {CONFIG['k_fold']})"
  )
  parser.add_argument(
    "--verbose",
    type = bool,
    default = CONFIG['verbose'],
    help = f"Verbose (default: {CONFIG['verbose']})"
  )
  parser.add_argument(
    "--dataset-base-directory",
    type = str,
    default = DATASET_BASE_DIRECTORY,
    help = f"Dataset Base Directory (default: {DATASET_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--output-base-directory",
    type = str,
    default = OUTPUT_BASE_DIRECTORY,
    help = f"Output Base Directory (default: {OUTPUT_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--dataset-info-file-name",
    type = str,
    default = DATASET_INFO_FILE_NAME,
    help = f"Dataset Info File Name (default: {DATASET_INFO_FILE_NAME})"
  )

  args = parser.parse_args()
  
  CONFIG['validation_fraction'] = args.validation_fraction
  CONFIG['test_fraction'] = args.test_fraction
  CONFIG['k_fold'] = args.k_fold
  CONFIG['verbose'] = args.verbose
  
  dataset_base_dir = args.dataset_base_directory
  output_base_dir = args.output_base_directory
  dataset_info_file = args.dataset_info_file_name
  
  CONFIG['dataset_info_csv'] = os.path.join(dataset_base_dir, dataset_info_file)
  CONFIG['directories']['save_base_directory'] = os.path.join(output_base_dir, 'create_splits')
  CONFIG['directories']['create_patches_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['extract_patches_directory'] = os.path.join(output_base_dir, 'extract_patches')
  CONFIG['directories']['features_pt_directory'] = os.path.join(output_base_dir, 'extract_features', 'pt_files')

def main():
  logger.draw_header("Create Dataset Splits")
  load_arguments()
  
  logger.info("Creating Dataset Splits...")
  start_time = time.time()

  logger.info("Creating Directories...")
  create_directories(CONFIG['directories'])

  show_configs()

  logger.info("Loading dataset from CSV...")
  df = pd.read_csv(CONFIG['dataset_info_csv'])
  if len(df) < 100:
    logger.error('Dataset is too small to create splits (size: {})', len(df))
    raise ValueError('Dataset is too small to create splits')
  logger.info("Loaded {} samples from dataset", len(df))

  logger.info("Initializing MIL Dataset...")
  dataset = GenericMILDataset(
    patches_dir = CONFIG['directories']['create_patches_directory'],
    extract_patches_dir = CONFIG['directories']['extract_patches_directory'],
    features_pt_directory = CONFIG['directories']['features_pt_directory'],
    csv_path = CONFIG['dataset_info_csv'],
    label_column = 'isup_grade',
    label_dict = { '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5 },
    verbose = CONFIG['verbose'],
  )

  num_slides_cls = np.array([len(cls_ids) for cls_ids in dataset.patient_cls_ids])
  logger.info("Creating {} splits with validation fraction {} and test fraction {}", 
             CONFIG['k_fold'], CONFIG['validation_fraction'], CONFIG['test_fraction'])
  
  dataset.create_splits(
    k = CONFIG['k_fold'],
    val_num = np.round(num_slides_cls * CONFIG['validation_fraction']).astype(int),
    test_num = np.round(num_slides_cls * CONFIG['test_fraction']).astype(int)
  )

  logger.info("Generating and saving splits:")
  for k in range(CONFIG['k_fold']):
    logger.text(f"> Processing fold {k+1}/{CONFIG['k_fold']}")
    dataset.set_splits()
    splits = dataset.return_splits()

    splits_path = os.path.join(CONFIG['directories']['save_base_directory'], 'splits_{}.csv'.format(k))
    save_splits(
      splits, 
      ['train', 'val', 'test'], 
      splits_path
    )
    
    bool_splits_path = os.path.join(CONFIG['directories']['save_base_directory'], 'splits_{}_boolean.csv'.format(k))
    save_splits(
      splits, 
      ['train', 'val', 'test'],
      bool_splits_path,
      boolean_style = True
    )

    descriptor_path = os.path.join(CONFIG['directories']['save_base_directory'], 'splits_{}_descriptor.csv'.format(k))
    descriptor_df = dataset.test_split_gen(return_descriptor = True, verbose = CONFIG['verbose'])
    descriptor_df.to_csv(descriptor_path)

  end_time = time.time()
  total_time = end_time - start_time

  logger.empty_line()
  logger.info("Total Processing Time: {:.2f} seconds ({:.2f} minutes)", total_time, total_time/60)
  logger.empty_line()
  logger.success("Created All Dataset Splits!")

if __name__ == '__main__':
  main()

[FILE-PATH: scripts/evaluate.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os 
import sys
import time
import argparse
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import OUTPUT_BASE_DIRECTORY, DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME
from constants.mil_models import MILModels
from constants.encoders import Encoders
from utils.helper import create_directories
from utils.data_loader import GenericMILDataset
from utils.train_engine import TrainEngine
from utils.file_utils import save_pkl, save_json
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'backbone': Encoders.RESNET50.value,
  'mil_model': MILModels.TRANS_MIL.value,
  'drop_out': True,
  'n_classes': 6,
  'learning_rate': 1e-4,
  'fold': 0,
  'patch_size': 512,
  'in_dim': 1024,
  'dataset_info_csv': os.path.join(DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME),
  'checkpoint_path': None, 
  'verbose': False,
  'save_predictions': True,
  'save_attention_maps': False,
  'directories': {
    'results_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'evaluation'),
    'train_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'train'),
    'create_splits_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_splits'),
    'create_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'extract_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_patches'),
    'features_pt_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features', 'pt_files'),
  }
}

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations:")
  logger.text(f"> Backbone: {CONFIG['backbone']}")
  logger.text(f"> MIL Model: {CONFIG['mil_model']}")
  logger.text(f"> Drop Out: {CONFIG['drop_out']}")
  logger.text(f"> Number of Classes: {CONFIG['n_classes']}")
  logger.text(f"> Learning Rate: {CONFIG['learning_rate']}")
  logger.text(f"> Fold: {CONFIG['fold']}")
  logger.text(f"> Patch Size: {CONFIG['patch_size']}")
  logger.text(f"> Input Dimension: {CONFIG['in_dim']}")
  logger.text(f"> Dataset Info CSV: {CONFIG['dataset_info_csv']}")
  logger.text(f"> Checkpoint Path: {CONFIG['checkpoint_path']}")
  logger.text(f"> Verbose: {CONFIG['verbose']}")
  logger.text(f"> Save Predictions: {CONFIG['save_predictions']}")
  logger.text(f"> Save Attention Maps: {CONFIG['save_attention_maps']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description="Evaluate MIL Models")
  parser.add_argument(
    "--backbone",
    type=str,
    default=CONFIG['backbone'],
    help=f"Backbone encoder model (default: {CONFIG['backbone']})"
  )
  parser.add_argument(
    "--mil-model",
    type=str,
    default=CONFIG['mil_model'],
    help=f"MIL model type (default: {CONFIG['mil_model']})"
  )
  parser.add_argument(
    "--drop-out",
    type=bool,
    default=CONFIG['drop_out'],
    help=f"Use dropout (default: {CONFIG['drop_out']})"
  )
  parser.add_argument(
    "--n-classes",
    type=int,
    default=CONFIG['n_classes'],
    help=f"Number of classes (default: {CONFIG['n_classes']})"
  )
  parser.add_argument(
    "--learning-rate",
    type=float,
    default=CONFIG['learning_rate'],
    help=f"Learning rate (default: {CONFIG['learning_rate']})"
  )
  parser.add_argument(
    "--fold",
    type=int,
    default=CONFIG['fold'],
    help=f"Fold to evaluate (default: {CONFIG['fold']})"
  )
  parser.add_argument(
    "--patch-size",
    type=int,
    default=CONFIG['patch_size'],
    help=f"Patch size (default: {CONFIG['patch_size']})"
  )
  parser.add_argument(
    "--in-dim",
    type=int,
    default=CONFIG['in_dim'],
    help=f"Input dimension (default: {CONFIG['in_dim']})"
  )
  parser.add_argument(
    "--dataset-info-csv",
    type=str,
    default=CONFIG['dataset_info_csv'],
    help=f"Dataset info CSV path (default: {CONFIG['dataset_info_csv']})"
  )
  parser.add_argument(
    "--checkpoint-path",
    type=str,
    default=CONFIG['checkpoint_path'],
    help="Path to model checkpoint (default: auto-generated based on other parameters)"
  )
  parser.add_argument(
    "--output-base-directory",
    type=str,
    default=OUTPUT_BASE_DIRECTORY,
    help=f"Output base directory (default: {OUTPUT_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--verbose",
    type=bool,
    default=CONFIG['verbose'],
    help=f"Verbose mode (default: {CONFIG['verbose']})"
  )
  parser.add_argument(
    "--save-predictions",
    type=bool,
    default=CONFIG['save_predictions'],
    help=f"Save individual predictions (default: {CONFIG['save_predictions']})"
  )
  parser.add_argument(
    "--save-attention-maps",
    type=bool,
    default=CONFIG['save_attention_maps'],
    help=f"Save attention maps for interpretability (default: {CONFIG['save_attention_maps']})"
  )
  
  args = parser.parse_args()
  
  CONFIG['backbone'] = args.backbone
  CONFIG['mil_model'] = args.mil_model
  CONFIG['drop_out'] = args.drop_out
  CONFIG['n_classes'] = args.n_classes
  CONFIG['learning_rate'] = args.learning_rate
  CONFIG['fold'] = args.fold
  CONFIG['patch_size'] = args.patch_size
  CONFIG['in_dim'] = args.in_dim
  CONFIG['dataset_info_csv'] = args.dataset_info_csv
  CONFIG['checkpoint_path'] = args.checkpoint_path
  CONFIG['verbose'] = args.verbose
  CONFIG['save_predictions'] = args.save_predictions
  CONFIG['save_attention_maps'] = args.save_attention_maps
  
  output_base_dir = args.output_base_directory
  
  CONFIG['directories']['results_directory'] = os.path.join(output_base_dir, 'evaluation')
  CONFIG['directories']['train_directory'] = os.path.join(output_base_dir, 'train')
  CONFIG['directories']['create_splits_directory'] = os.path.join(output_base_dir, 'create_splits')
  CONFIG['directories']['create_patches_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['extract_patches_directory'] = os.path.join(output_base_dir, 'extract_patches')
  CONFIG['directories']['features_pt_directory'] = os.path.join(output_base_dir, 'extract_features', 'pt_files')

  if CONFIG['checkpoint_path'] is None:
    CONFIG['checkpoint_path'] = os.path.join(
      CONFIG['directories']['train_directory'],
      f"s_{CONFIG['fold']}_checkpoint.pt"
    )

def plot_confusion_matrix(cm, classes, save_path):
  plt.figure(figsize=(10, 8))
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
  disp.plot(cmap=plt.cm.Blues, values_format='d')
  plt.title('Confusion Matrix')
  plt.savefig(save_path, bbox_inches='tight')
  plt.close()

def main():
  logger.draw_header("Evaluate MIL Model")
  load_arguments()
  
  logger.info("Evaluating MIL model...")
  start_time = time.time()

  logger.info("Creating directories...")
  create_directories(CONFIG['directories'])
  
  model_output_dir = os.path.join(
    CONFIG['directories']['results_directory'],
    f"{CONFIG['mil_model']}_{CONFIG['backbone']}_fold_{CONFIG['fold']}"
  )
  os.makedirs(model_output_dir, exist_ok=True)

  show_configs()

  if not os.path.exists(CONFIG['checkpoint_path']):
    logger.error(f"Checkpoint not found at: {CONFIG['checkpoint_path']}")
    logger.error("Please provide a valid checkpoint path or ensure the model was trained")
    return
  
  logger.info(f"Loading dataset for fold {CONFIG['fold']}...")
  dataset = GenericMILDataset(
    patches_dir = CONFIG['directories']['create_patches_directory'],
    extract_patches_dir = CONFIG['directories']['extract_patches_directory'],
    features_pt_directory = CONFIG['directories']['features_pt_directory'],
    csv_path = CONFIG['dataset_info_csv'],
    label_column = 'isup_grade',
    label_dict = { '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5 },
    verbose = CONFIG['verbose'],
  )
  
  logger.info(f"Setting up dataset split for fold {CONFIG['fold']}...")
  dataset_split = dataset.return_splits(
    CONFIG['backbone'],
    CONFIG['patch_size'], 
    from_id = False, 
    csv_path='{}/splits_{}.csv'.format(CONFIG['directories']['create_splits_directory'], CONFIG['fold'])
  )
  
  drop_out = 0.25 if CONFIG['drop_out'] else 0.0
  logger.info("Initializing evaluation engine...")
  train_engine = TrainEngine(
    datasets = dataset_split,
    fold = CONFIG['fold'],
    drop_out = drop_out,
    result_directory = CONFIG['directories']['train_directory'],
    mil_model_name = CONFIG['mil_model'],
    learning_rate = CONFIG['learning_rate'],
    max_epochs = 1,
    in_dim = CONFIG['in_dim'],
    n_classes = CONFIG['n_classes'],
    verbose = CONFIG['verbose'],
  )

  logger.info(f"Evaluating model from checkpoint: {CONFIG['checkpoint_path']}")
  _, test_error, test_auc, acc_logger, df, test_f1, test_metrics = train_engine.eval_model(CONFIG['checkpoint_path'])
  
  if CONFIG['save_predictions']:
    predictions_path = os.path.join(model_output_dir, "predictions.csv")
    logger.info(f"Saving predictions to {predictions_path}")
    df.to_csv(predictions_path, index=False)
  
  metrics_path = os.path.join(model_output_dir, "metrics.json")
  metrics_to_save = {
    'accuracy': 1 - test_error,
    'error': test_error,
    'auc': test_auc,
    'f1_score': test_f1,
    'precision': test_metrics['precision_macro'],
    'recall': test_metrics['recall_macro'],
    'cohens_kappa': test_metrics['cohens_kappa'],
  }
  
  metrics_to_save['class_metrics'] = {}
  for i in range(CONFIG['n_classes']):
    class_metrics = {}
    acc, correct, count = acc_logger.get_summary(i)
    class_metrics['accuracy'] = acc
    class_metrics['count'] = count
    
    if 'f1_per_class' in test_metrics and i < len(test_metrics['f1_per_class']):
      class_metrics['f1'] = float(test_metrics['f1_per_class'][i])
    
    if 'precision_per_class' in test_metrics and i < len(test_metrics['precision_per_class']):
      class_metrics['precision'] = float(test_metrics['precision_per_class'][i])
    
    if 'recall_per_class' in test_metrics and i < len(test_metrics['recall_per_class']):
      class_metrics['recall'] = float(test_metrics['recall_per_class'][i])
    
    metrics_to_save['class_metrics'][f'class_{i}'] = class_metrics
  
  logger.info(f"Saving metrics to {metrics_path}")
  save_json(metrics_path, metrics_to_save)
  
  if 'confusion_matrix' in test_metrics:
    cm_path = os.path.join(model_output_dir, "confusion_matrix.png")
    logger.info(f"Saving confusion matrix to {cm_path}")
    class_names = [str(i) for i in range(CONFIG['n_classes'])]
    plot_confusion_matrix(test_metrics['confusion_matrix'], class_names, cm_path)

  full_metrics_path = os.path.join(model_output_dir, "full_metrics.pkl")
  logger.info(f"Saving full metrics to {full_metrics_path}")
  save_pkl(full_metrics_path, test_metrics)
  
  end_time = time.time()
  total_time = end_time - start_time
  
  logger.empty_line()
  logger.info("===== Evaluation Results Summary =====")
  logger.info(f"Model: {CONFIG['mil_model']}, Backbone: {CONFIG['backbone']}, Fold: {CONFIG['fold']}")
  logger.success(f"Accuracy: {1 - test_error:.4f}")
  logger.success(f"AUC: {test_auc:.4f}")
  logger.success(f"F1 Score: {test_f1:.4f}")
  logger.success(f"Precision: {test_metrics['precision_macro']:.4f}")
  logger.success(f"Recall: {test_metrics['recall_macro']:.4f}")
  logger.success(f"Cohen's Kappa: {test_metrics['cohens_kappa']:.4f}")
  
  logger.info("Class-specific results:")
  for i in range(CONFIG['n_classes']):
    acc, correct, count = acc_logger.get_summary(i)
    logger.info(f"  Class {i}: Accuracy {acc:.4f}, Correct {correct}/{count}")
  
  logger.info(f"Evaluation completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
  logger.success("Evaluation completed successfully!")

if __name__ == '__main__':
  main()


[FILE-PATH: scripts/extract_features.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import torch
import time
import h5py
import pandas as pd
import random
import argparse
from PIL import Image
from multiprocessing import Process
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME, OUTPUT_BASE_DIRECTORY
from constants.encoders import Encoders
from encoders import get_encoder, get_custom_transformer
from utils.helper import create_directories
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'encoder': Encoders.RESNET50.value,
  'batch_size': 128,
  'patch_size': 512,
  'dataset_info_csv': os.path.join(DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME),
  'processed_dataset_info_csv': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features', 'processed_dataset_info.csv'),
  'directories': {
    'patches_h5_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'extract_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_patches'),
    'save_base_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features'),
    'features_pt_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features', 'pt_files'),
  },
  'verbose': False
}

def prepare_csv(
  label_csv_path: str,
  patches_h5_path: str,
  csv_save_path: str
) -> None:
  random.seed(0)
  data_items = []
  
  df = pd.read_csv(label_csv_path)
  for idx, row in df.iterrows():
    slide_id = row['image_id']
    p = os.path.join(patches_h5_path, f'{slide_id}.h5')
    if os.path.exists(p):
      data_items.append([slide_id, row['isup_grade']])

  random.shuffle(data_items)

  new_df_columns = ['dir', 'case_id', 'slide_id', 'label']
  new_df = pd.DataFrame(columns = new_df_columns)

  rows = []
  for slide_id, isup in data_items:
    rows.append({
      'dir': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches'),
      'case_id': slide_id,
      'slide_id': slide_id,
      'label': isup
    })
  
  new_df = pd.concat([new_df, pd.DataFrame(rows)], ignore_index=True)
  new_df.to_csv(csv_save_path, index = False)

class DatasetAllBags(Dataset):
	def __init__(self, csv_path):
		self.df = pd.read_csv(
      csv_path,
      dtype = { 'case_id': str, 'slide_id': str }
    )
	
	def __len__(self):
		return len(self.df)

	def __getitem__(self, idx):
		return self.df['dir'][idx], self.df['slide_id'][idx]

class PatchDataset(Dataset):
  def __init__(
    self,
    img_root,
    patch_h5_path,
    transform = None
  ) -> None:
    super().__init__()

    self.img_root = img_root
    self.transform = transform
    
    with h5py.File(patch_h5_path, 'r') as hf:
      self.coords = hf['coords'][()]
    
    actual_files = os.listdir(img_root)
    assert len(actual_files) + 1 >= len(self.coords), \
      'real patch {} not match h5 patch number {}'.format(len(actual_files), len(self.coords))

  def __len__(self):
    return len(self.coords)

  def __getitem__(self, index):
    x, y = self.coords[index]
    
    img_name = f'{x}_{y}_{CONFIG["patch_size"]}_{CONFIG["patch_size"]}.jpg'
    image_path = os.path.join(self.img_root, img_name)
    img = Image.open(image_path)
    
    if self.transform is not None:
      img = self.transform(img)
      
    return img

def save_feature(path, feature):
  s = time.time()
  torch.save(feature, path)
  e = time.time()
  if CONFIG['verbose']: logger.info(f'Feature Sucessfully Saved At: {path}, Time: {e-s:.1f}s')

def save_feature_subprocess(path, feature):
  kwargs = {
    'feature': feature, 
    'path': path
  }
  
  process = Process(target = save_feature, kwargs = kwargs)
  process.start()

def light_compute_w_loader(
  loader,
  encoder,
  device,
  print_every = 20,
):
  features_list = []
  _start_time = time.time()
  for count, batch in enumerate(loader):
    with torch.no_grad():	
      if count % print_every == 0:
        batch_time = time.time()
        if CONFIG['verbose']: 
          logger.info(f'batch {count}/{len(loader)}, {count * len(batch)} files processed, used_time: {batch_time - _start_time:.2f}s')

      batch = batch.to(device, non_blocking=True)
      features = encoder(batch)
      features = features.cpu()
      features_list.append(features)

  features = torch.cat(features_list, dim=0)
  return features

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations;")
  logger.text(f"> Encoder: {CONFIG['encoder']}")
  logger.text(f"> Batch Size: {CONFIG['batch_size']}")
  logger.text(f"> Patch Size: {CONFIG['patch_size']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description = "Extract Features From Patches")
  parser.add_argument(
    "--encoder",
    type = str,
    default = CONFIG['encoder'],
    help = f"Encoder Model Name (default: {CONFIG['encoder']}) ({', '.join([e.value for e in Encoders])})"
  )
  parser.add_argument(
    "--batch-size",
    type = int,
    default = CONFIG['batch_size'],
    help = f"Batch Size (default: {CONFIG['batch_size']})"
  )
  parser.add_argument(
    "--patch-size",
    type = int,
    default = CONFIG['patch_size'],
    help = f"Patch Size (default: {CONFIG['patch_size']})"
  )
  parser.add_argument(
    "--verbose",
    type = bool,
    default = CONFIG['verbose'],
    help = f"Verbose (default: {CONFIG['verbose']})"
  )
  parser.add_argument(
    "--dataset-base-directory",
    type = str,
    default = DATASET_BASE_DIRECTORY,
    help = f"Dataset Base Directory (default: {DATASET_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--dataset-info-file-name",
    type = str,
    default = DATASET_INFO_FILE_NAME,
    help = f"Dataset Info File Name (default: {DATASET_INFO_FILE_NAME})"
  )
  parser.add_argument(
    "--output-base-directory",
    type = str,
    default = OUTPUT_BASE_DIRECTORY,
    help = f"Output Base Directory (default: {OUTPUT_BASE_DIRECTORY})"
  )

  args = parser.parse_args()

  if args.encoder not in [e.value for e in Encoders]:
    raise ValueError(f"Invalid Encoder: {args.encoder}")
  
  CONFIG['encoder'] = args.encoder
  CONFIG['batch_size'] = args.batch_size
  CONFIG['patch_size'] = args.patch_size
  CONFIG['verbose'] = args.verbose
  
  dataset_base_dir = args.dataset_base_directory
  dataset_info_file = args.dataset_info_file_name
  output_base_dir = args.output_base_directory
  
  CONFIG['dataset_info_csv'] = os.path.join(dataset_base_dir, dataset_info_file)
  CONFIG['processed_dataset_info_csv'] = os.path.join(output_base_dir, 'extract_features', 'processed_dataset_info.csv')
  CONFIG['directories']['patches_h5_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['extract_patches_directory'] = os.path.join(output_base_dir, 'extract_patches')
  CONFIG['directories']['save_base_directory'] = os.path.join(output_base_dir, 'extract_features')
  CONFIG['directories']['features_pt_directory'] = os.path.join(output_base_dir, 'extract_features', 'pt_files')

def main():
  logger.draw_header("Extract Features From Patches")
  load_arguments()
  
  logger.info("Extracting Features From Patches...")
  process_start_time = time.time()

  logger.info("Creating Directories...")
  create_directories(CONFIG['directories'])

  logger.info("Preparing Processed Dataset Info CSV...")
  prepare_csv(
    label_csv_path = CONFIG['dataset_info_csv'],
    patches_h5_path = CONFIG['directories']['patches_h5_directory'],
    csv_save_path = CONFIG['processed_dataset_info_csv']
  )

  show_configs()
  
  dataset_bags = DatasetAllBags(CONFIG['processed_dataset_info_csv'])

  encoder_pt_path = os.path.join(CONFIG['directories']['features_pt_directory'], CONFIG['encoder'])
  os.makedirs(encoder_pt_path, exist_ok = True)
  destination_files = os.listdir(encoder_pt_path)

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  logger.info(f'Using Device: {device}' + (f' ({torch.cuda.device_count()} GPUs)' if torch.cuda.is_available() else ""))

  encoder = get_encoder(CONFIG['encoder'], device, torch.cuda.device_count())
  custom_transformer = get_custom_transformer(CONFIG['encoder'])

  exiting_indexes = []
  for bag_candidate_index in range(len(dataset_bags)):
    dataset_dir, slide_id = dataset_bags[bag_candidate_index]
    bag_name = f'{slide_id}.h5'
    h5_file_path = os.path.join(CONFIG['directories']['patches_h5_directory'], bag_name)
    if not os.path.exists(h5_file_path):
      if CONFIG['verbose']: logger.error(f'{h5_file_path} does not exist...')
      continue
    elif f'{slide_id}.pt' in destination_files:
      if CONFIG['verbose']: logger.info(f'Skipped {slide_id}')
      continue
    else:
      exiting_indexes.append(bag_candidate_index)

  total_features = 0
  total_slides_processed = 0

  logger.empty_line()
  for bag_candidate_index in tqdm(exiting_indexes, desc="Extracting Features", unit="slide"):
    dataset_dir, slide_id = dataset_bags[bag_candidate_index]
    if CONFIG['verbose']: logger.info(f'\nSlide ID: {slide_id}')
    
    output_feature_path = os.path.join(encoder_pt_path, f'{slide_id}.pt')
    h5_file_path = os.path.join(CONFIG['directories']['patches_h5_directory'], f'{slide_id}.h5')

    one_slide_start = time.time()

    if not os.path.exists(h5_file_path):
      if CONFIG['verbose']: logger.error(f'{h5_file_path} does not exist, skipping...')
      continue

    images_path = os.path.join(CONFIG['directories']['extract_patches_directory'], slide_id)
    patch_dataset = PatchDataset(images_path, h5_file_path, transform = custom_transformer)
    loader = DataLoader(
      patch_dataset,
      batch_size = CONFIG['batch_size'],
      shuffle = False,
      num_workers = os.cpu_count()
    )

    # created a temporary file to help other processes
    with open(f'{output_feature_path}.partial', 'w') as f: 
      f.write('')

    features = light_compute_w_loader(
      loader = loader,
      encoder = encoder,
      device = device
    )

    save_feature_subprocess(output_feature_path, features)
    os.remove(f'{output_feature_path}.partial')
    
    if CONFIG['verbose']: logger.info(f'Coords Shape: {features.shape}')
    if CONFIG['verbose']: logger.info(f'Elapsed Time: {time.time() - one_slide_start:.2f}s')
    
    total_features += features.shape[0]
    total_slides_processed += 1

  end_time = time.time()
  total_time = end_time - process_start_time
  avg_time_per_slide = total_time / total_slides_processed if total_slides_processed > 0 else 0

  logger.empty_line()
  logger.info("Total Slides Processed: {}", total_slides_processed)
  logger.info("Total Processing Time: {:.2f} minutes", total_time / 60)
  logger.info("Average Time Per Slide: {:.2f} seconds", avg_time_per_slide)
  logger.empty_line()
  logger.success("Extracted All Features!")

if __name__ == '__main__':
  try: main()
  except Exception as e: logger.error(e)

[FILE-PATH: scripts/extract_patches.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import h5py
import glob
import time
import argparse
import openslide
from multiprocessing.pool import Pool
from tqdm import tqdm

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import DATASET_BASE_DIRECTORY, DATASET_SLIDES_FOLDER_NAME, OUTPUT_BASE_DIRECTORY
from utils.helper import create_directories
from utils.wsi_core.whole_slide_image import ImgReader
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'slides_format': 'tiff',
  'patch_level': 0,
  'patch_size': 512,
  'num_workers': os.cpu_count(),
  'directories': {
    'slides_directory': os.path.join(DATASET_BASE_DIRECTORY, DATASET_SLIDES_FOLDER_NAME),
    'patches_h5_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'save_base_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_patches'),
  },
  'verbose': False
}

def get_wsi_handle(
  wsi_path,
  verbose = False
):
  if not os.path.exists(wsi_path):
    raise FileNotFoundError(f'{wsi_path} is not found')

  postfix = wsi_path.split('.')[-1]
  if postfix.lower() in ['svs', 'mrxs']:
    handle = openslide.OpenSlide(wsi_path)
  else:
    handle = ImgReader(wsi_path, verbose = verbose)

  return handle

def read_images(args):
  h5_path, save_root, wsi_path, size, level, verbose = args
  if wsi_path is None: return 0

  try: 
    h5 = h5py.File(h5_path)
    os.makedirs(save_root, exist_ok=True)
  except:
    if verbose: logger.error("{} is not readable....", h5_path)
    return 0
  
  if len(h5['coords']) == len(os.listdir(save_root)): 
    return len(h5['coords'])
  
  wsi_handle = get_wsi_handle(wsi_path, verbose)
  patch_count = 0
  for x, y in h5['coords']:
    patch_path = os.path.join(save_root, f'{x}_{y}_{size[0]}_{size[1]}.jpg')
    if os.path.exists(patch_path):
      patch_count += 1
      continue

    try:
      img = wsi_handle.read_region((x, y), level, size).convert('RGB')
      img.save(patch_path)
      patch_count += 1
    except:
      if verbose: logger.error("Failed to read: {}, {}, {}", wsi_path, x, y)

  return patch_count

def get_wsi_path(
  wsi_root,
  h5_files,
  wsi_format
):
  kv = {}
  all_paths = glob.glob(os.path.join(wsi_root, f'*.{wsi_format}'), recursive=True)
  for h5_file in h5_files:
    slide_id = os.path.basename(h5_file).split('.')[0]
    for path in all_paths:
      if slide_id in path:
        kv[h5_file] = path
        break

  wsi_paths = [kv[h5_file] for h5_file in h5_files]
  return wsi_paths

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations;")
  logger.text(f"> Slides Format: {CONFIG['slides_format']}")
  logger.text(f"> Patch Level: {CONFIG['patch_level']}")
  logger.text(f"> Patch Size: {CONFIG['patch_size']}")
  logger.text(f"> Number of Threads: {CONFIG['num_workers']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description = "Extract Patches From Slides")
  parser.add_argument(
    "--slides-format",
    type = str,
    default = CONFIG['slides_format'],
    help = f"Slides Format (default: {CONFIG['slides_format']})"
  )
  parser.add_argument(
    "--patch-level",
    type = int,
    default = CONFIG['patch_level'],
    help = f"Patch Level (default: {CONFIG['patch_level']})"
  )
  parser.add_argument(
    "--patch-size",
    type = int,
    default = CONFIG['patch_size'],
    help = f"Patch Size (default: {CONFIG['patch_size']})"
  )
  parser.add_argument(
    "--num-workers",
    type = int,
    default = CONFIG['num_workers'],
    help = f"Number of Workers (default: {CONFIG['num_workers']})"
  )
  parser.add_argument(
    "--verbose",
    type = bool,
    default = CONFIG['verbose'],
    help = f"Verbose (default: {CONFIG['verbose']})"
  )
  parser.add_argument(
    "--dataset-base-directory",
    type = str,
    default = DATASET_BASE_DIRECTORY,
    help = f"Dataset Base Directory (default: {DATASET_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--dataset-slides-folder-name",
    type = str,
    default = DATASET_SLIDES_FOLDER_NAME,
    help = f"Dataset Slides Folder Name (default: {DATASET_SLIDES_FOLDER_NAME})"
  )
  parser.add_argument(
    "--output-base-directory",
    type = str,
    default = OUTPUT_BASE_DIRECTORY,
    help = f"Output Base Directory (default: {OUTPUT_BASE_DIRECTORY})"
  )

  args = parser.parse_args()
  
  CONFIG['slides_format'] = args.slides_format
  CONFIG['patch_level'] = args.patch_level
  CONFIG['patch_size'] = args.patch_size
  CONFIG['num_workers'] = args.num_workers
  CONFIG['verbose'] = args.verbose
  
  dataset_base_dir = args.dataset_base_directory
  dataset_slides_folder = args.dataset_slides_folder_name
  output_base_dir = args.output_base_directory
  
  CONFIG['directories']['slides_directory'] = os.path.join(dataset_base_dir, dataset_slides_folder)
  CONFIG['directories']['patches_h5_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['save_base_directory'] = os.path.join(output_base_dir, 'extract_patches')

def main():
  logger.draw_header("Extract Patches From Slides")
  load_arguments()
  
  logger.info("Extracting Patches From Slides...")
  start_time = time.time()

  logger.info("Creating Directories...")
  create_directories(CONFIG['directories'])

  show_configs()
  
  h5_files = glob.glob(os.path.join(CONFIG['directories']['patches_h5_directory'], '*.h5'))
  wsi_paths = get_wsi_path(
    CONFIG['directories']['slides_directory'],
    h5_files,
    CONFIG['slides_format']
  )

  args = [
    (
      h5_file, 
      os.path.join(
        CONFIG['directories']['save_base_directory'], 
        os.path.basename(h5_file).split('.')[0]
      ),
      wsi_path,
      (CONFIG['patch_size'], CONFIG['patch_size']),
      CONFIG['patch_level'],
      CONFIG['verbose']
    )
    for h5_file, wsi_path in zip(h5_files, wsi_paths)
  ]

  logger.info("Using {} workers to extract patches from slides...\n", CONFIG["num_workers"])
  with Pool(CONFIG['num_workers']) as p:
    patch_counts = list(tqdm(
      p.imap_unordered(read_images, args),
      total=len(args),
      desc="Extracting Patches",
      unit="slide"
    ))
  
  end_time = time.time()
  total_time = end_time - start_time
  slide_count = len(args)
  valid_counts = [count for count in patch_counts if count is not None]

  total_patches = sum(valid_counts)
  avg_patches_per_slide = total_patches / slide_count if slide_count > 0 else 0
  avg_time_per_slide = total_time / slide_count if slide_count > 0 else 0

  logger.empty_line()
  logger.info("Total Slides Processed: {}", slide_count)
  logger.info("Total Patches Extracted: {}", total_patches)
  logger.info("Average Patches Per Slide: {:.2f}", avg_patches_per_slide)
  logger.info("Total Processing Time: {:.2f} seconds ({:.2f} minutes)", total_time, total_time/60)
  logger.info("Average Time Per Slide: {:.2f} seconds", avg_time_per_slide)
  logger.empty_line()
  logger.success("Extracted All Patches From Slides!")

if __name__ == '__main__':
  main()

[FILE-PATH: scripts/train.py]
[CODE]
import warnings
warnings.filterwarnings('ignore')

import os 
import sys
import pandas as pd
import numpy as np
import time
import argparse
import random
import torch

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from constants.misc import OUTPUT_BASE_DIRECTORY, DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME
from constants.mil_models import MILModels
from constants.encoders import Encoders
from utils.helper import create_directories
from utils.data_loader import GenericMILDataset
from utils.train_engine import TrainEngine
from utils.file_utils import save_pkl
from utils.logger import Logger

logger = Logger()

CONFIG = {
  'backbone': Encoders.RESNET50.value,
  'mil_model': MILModels.TRANS_MIL.value,
  'drop_out': True,
  'n_classes': 6,
  'learning_rate': 1e-4,
  'k_fold': 1,
  'patch_size': 512,
  'in_dim': 1024,
  'max_epochs': 10,
  'dataset_info_csv': os.path.join(DATASET_BASE_DIRECTORY, DATASET_INFO_FILE_NAME),
  'directories': {
    'save_base_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'train'),
    'create_splits_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_splits'),
    'create_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'create_patches', 'patches'),
    'extract_patches_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_patches'),
    'features_pt_directory': os.path.join(OUTPUT_BASE_DIRECTORY, 'extract_features', 'pt_files'),
  },
  'verbose': False,
}

def seed_torch(seed = 1):
  random.seed(seed)
  os.environ['PYTHONHASHSEED'] = str(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if we are using multi-GPU.
  torch.backends.cudnn.benchmark = True
  torch.backends.cudnn.deterministic = True

def show_configs():
  logger.empty_line()
  logger.info("Using Configurations:")
  logger.text(f"> Backbone: {CONFIG['backbone']}")
  logger.text(f"> MIL Model: {CONFIG['mil_model']}")
  logger.text(f"> Drop Out: {CONFIG['drop_out']}")
  logger.text(f"> Number of Classes: {CONFIG['n_classes']}")
  logger.text(f"> Learning Rate: {CONFIG['learning_rate']}")
  logger.text(f"> K-Fold: {CONFIG['k_fold']}")
  logger.text(f"> Patch Size: {CONFIG['patch_size']}")
  logger.text(f"> Input Dimension: {CONFIG['in_dim']}")
  logger.text(f"> Max Epochs: {CONFIG['max_epochs']}")
  logger.text(f"> Dataset Info CSV: {CONFIG['dataset_info_csv']}")
  logger.text(f"> Verbose: {CONFIG['verbose']}")
  logger.empty_line()

def load_arguments():
  parser = argparse.ArgumentParser(description="Train MIL Models")
  parser.add_argument(
    "--backbone",
    type=str,
    default=CONFIG['backbone'],
    help=f"Backbone encoder model (default: {CONFIG['backbone']})"
  )
  parser.add_argument(
    "--mil-model",
    type=str,
    default=CONFIG['mil_model'],
    help=f"MIL model type (default: {CONFIG['mil_model']})"
  )
  parser.add_argument(
    "--drop-out",
    type=bool,
    default=CONFIG['drop_out'],
    help=f"Use dropout (default: {CONFIG['drop_out']})"
  )
  parser.add_argument(
    "--n-classes",
    type=int,
    default=CONFIG['n_classes'],
    help=f"Number of classes (default: {CONFIG['n_classes']})"
  )
  parser.add_argument(
    "--learning-rate",
    type=float,
    default=CONFIG['learning_rate'],
    help=f"Learning rate (default: {CONFIG['learning_rate']})"
  )
  parser.add_argument(
    "--k-fold",
    type=int,
    default=CONFIG['k_fold'],
    help=f"Number of folds (default: {CONFIG['k_fold']})"
  )
  parser.add_argument(
    "--patch-size",
    type=int,
    default=CONFIG['patch_size'],
    help=f"Patch size (default: {CONFIG['patch_size']})"
  )
  parser.add_argument(
    "--in-dim",
    type=int,
    default=CONFIG['in_dim'],
    help=f"Input dimension (default: {CONFIG['in_dim']})"
  )
  parser.add_argument(
    "--max-epochs",
    type=int,
    default=CONFIG['max_epochs'],
    help=f"Maximum epochs (default: {CONFIG['max_epochs']})"
  )
  parser.add_argument(
    "--dataset-info-csv",
    type=str,
    default=CONFIG['dataset_info_csv'],
    help=f"Dataset info CSV path (default: {CONFIG['dataset_info_csv']})"
  )
  parser.add_argument(
    "--output-base-directory",
    type=str,
    default=OUTPUT_BASE_DIRECTORY,
    help=f"Output base directory (default: {OUTPUT_BASE_DIRECTORY})"
  )
  parser.add_argument(
    "--verbose",
    type=bool,
    default=CONFIG['verbose'],
    help=f"Verbose mode (default: {CONFIG['verbose']})"
  )
  
  args = parser.parse_args()
  
  CONFIG['backbone'] = args.backbone
  CONFIG['mil_model'] = args.mil_model
  CONFIG['drop_out'] = args.drop_out
  CONFIG['n_classes'] = args.n_classes
  CONFIG['learning_rate'] = args.learning_rate
  CONFIG['k_fold'] = args.k_fold
  CONFIG['patch_size'] = args.patch_size
  CONFIG['in_dim'] = args.in_dim
  CONFIG['max_epochs'] = args.max_epochs
  CONFIG['dataset_info_csv'] = args.dataset_info_csv
  CONFIG['verbose'] = args.verbose
  
  output_base_dir = args.output_base_directory
  
  CONFIG['directories']['save_base_directory'] = os.path.join(output_base_dir, 'train')
  CONFIG['directories']['create_splits_directory'] = os.path.join(output_base_dir, 'create_splits')
  CONFIG['directories']['create_patches_directory'] = os.path.join(output_base_dir, 'create_patches', 'patches')
  CONFIG['directories']['extract_patches_directory'] = os.path.join(output_base_dir, 'extract_patches')
  CONFIG['directories']['features_pt_directory'] = os.path.join(output_base_dir, 'extract_features', 'pt_files')

def main():
  logger.draw_header("Train MIL Model")
  load_arguments()
  
  logger.info("Training MIL model...")
  start_time = time.time()

  logger.info("Creating Directories...")
  create_directories(CONFIG['directories'])

  show_configs()

  folds = np.arange(0, CONFIG['k_fold'])
  
  logger.info("Loading dataset...")
  dataset = GenericMILDataset(
    patches_dir = CONFIG['directories']['create_patches_directory'],
    extract_patches_dir = CONFIG['directories']['extract_patches_directory'],
    features_pt_directory = CONFIG['directories']['features_pt_directory'],
    csv_path = CONFIG['dataset_info_csv'],
    label_column = 'isup_grade',
    label_dict = { '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5 },
    verbose = CONFIG['verbose'],
  )
  
  all_test_auc = []
  all_val_auc = []
  all_test_acc = []
  all_val_acc = []
  all_test_f1 = []
  all_val_f1 = []
  all_test_precision = []
  all_val_precision = []
  all_test_recall = []
  all_val_recall = []
  all_test_kappa = []
  all_val_kappa = []

  for fold in folds:
    logger.info("Training fold {}/{}...", fold+1, CONFIG['k_fold'])
    fold_start_time = time.time()
    
    seed_torch()
    
    logger.info("Setting up dataset split for fold {}...", fold)
    dataset_split = dataset.return_splits(
      CONFIG['backbone'],
      CONFIG['patch_size'], 
      from_id = False, 
      csv_path='{}/splits_{}.csv'.format(CONFIG['directories']['create_splits_directory'], fold)
    )

    drop_out = 0.25 if CONFIG['drop_out'] else 0.0
    logger.info("Initializing training engine...")
    train_engine = TrainEngine(
      datasets = dataset_split,
      fold = fold,
      drop_out = drop_out,
      result_directory = CONFIG['directories']['save_base_directory'],
      mil_model_name = CONFIG['mil_model'],
      learning_rate = CONFIG['learning_rate'],
      max_epochs = CONFIG['max_epochs'],
      in_dim = CONFIG['in_dim'],
      n_classes = CONFIG['n_classes'],
      verbose = CONFIG['verbose'],
    )

    logger.info("Starting model training for fold {}...", fold)
    results, test_auc, val_auc, test_acc, val_acc, test_f1, val_f1, test_metrics, val_metrics = train_engine.train_model(fold)
    all_test_auc.append(test_auc)
    all_val_auc.append(val_auc)
    all_test_acc.append(test_acc)
    all_val_acc.append(val_acc)
    all_test_f1.append(test_f1)
    all_val_f1.append(val_f1)
    all_test_precision.append(test_metrics['precision_macro'])
    all_val_precision.append(val_metrics['precision_macro'])
    all_test_recall.append(test_metrics['recall_macro'])
    all_val_recall.append(val_metrics['recall_macro'])
    all_test_kappa.append(test_metrics['cohens_kappa'])
    all_val_kappa.append(val_metrics['cohens_kappa'])

    fold_metrics = {
      'test': test_metrics,
      'val': val_metrics
    }
    save_pkl(os.path.join(CONFIG['directories']['save_base_directory'], 'split_{}_metrics.pkl'.format(fold)), fold_metrics)
    
    filename = os.path.join(CONFIG['directories']['save_base_directory'], 'split_{}_results.pkl'.format(fold))
    save_pkl(filename, results)
    
    fold_time = time.time() - fold_start_time
    logger.info("Fold {}/{} completed in {:.2f} seconds ({:.2f} minutes)", 
                fold+1, CONFIG['k_fold'], fold_time, fold_time/60)
    logger.info("Fold {} results - Test AUC: {:.4f}, Val AUC: {:.4f}", 
                fold, test_auc, val_auc)
    logger.empty_line()

  logger.info("Saving summary results...")
  final_df = pd.DataFrame({
    'folds': folds,
    'test_auc': all_test_auc, 
    'val_auc': all_val_auc,
    'test_acc': all_test_acc,
    'val_acc': all_val_acc,
    'test_f1': all_test_f1,
    'val_f1': all_val_f1,
    'test_precision': all_test_precision,
    'val_precision': all_val_precision,
    'test_recall': all_test_recall,
    'val_recall': all_val_recall,
    'test_kappa': all_test_kappa,
    'val_kappa': all_val_kappa
  })

  if len(folds) != CONFIG['k_fold']:
    save_name = 'summary_partial_{}_{}.csv'.format(folds[0], folds[-1])
  else:
    save_name = 'summary.csv'
  
  final_df.to_csv(os.path.join(CONFIG['directories']['save_base_directory'], save_name))
  
  total_time = time.time() - start_time
  logger.empty_line()
  logger.info("Training completed!")
  logger.info("Total folds processed: {}", len(folds))
  
  logger.info("===== Performance Metrics Summary =====")
  logger.info("--- Test Metrics ---")
  logger.info("Average Accuracy: {:.4f}", np.mean(all_test_acc))
  logger.info("Average AUC: {:.4f}", np.mean(all_test_auc))
  logger.info("Average F1 Score: {:.4f}", np.mean(all_test_f1))
  logger.info("Average Precision: {:.4f}", np.mean(all_test_precision))
  logger.info("Average Recall: {:.4f}", np.mean(all_test_recall))
  logger.info("Average Cohen's Kappa: {:.4f}", np.mean(all_test_kappa))
  
  logger.info("--- Validation Metrics ---")
  logger.info("Average Accuracy: {:.4f}", np.mean(all_val_acc))
  logger.info("Average AUC: {:.4f}", np.mean(all_val_auc))
  logger.info("Average F1 Score: {:.4f}", np.mean(all_val_f1))
  logger.info("Average Precision: {:.4f}", np.mean(all_val_precision))
  logger.info("Average Recall: {:.4f}", np.mean(all_val_recall))
  logger.info("Average Cohen's Kappa: {:.4f}", np.mean(all_val_kappa))
  
  logger.info("Total processing time: {:.2f} seconds ({:.2f} minutes)", 
              total_time, total_time/60)
  logger.success("MIL model training completed successfully!")

if __name__ == '__main__':
  main()

[FILE-PATH: utils/common_utils.py]
[CODE]
import numpy as np
import torch


class AccuracyLogger(object):
  def __init__(self, n_classes):
    super(AccuracyLogger, self).__init__()
    self.n_classes = n_classes
    self.initialize()

  def initialize(self):
    self.data = [{"count": 0, "correct": 0} for i in range(self.n_classes)]
  
  def log(self, Y_hat, Y):
    Y_hat = int(Y_hat)
    Y = int(Y)
    self.data[Y]["count"] += 1
    self.data[Y]["correct"] += (Y_hat == Y)
  
  def log_batch(self, Y_hat, Y):
    Y_hat = np.array(Y_hat).astype(int)
    Y = np.array(Y).astype(int)
    for label_class in np.unique(Y):
      cls_mask = Y == label_class
      self.data[label_class]["count"] += cls_mask.sum()
      self.data[label_class]["correct"] += (Y_hat[cls_mask] == Y[cls_mask]).sum()
  
  def get_summary(self, c):
    count = self.data[c]["count"] 
    correct = self.data[c]["correct"]
    if count == 0: acc = None
    else: acc = float(correct) / count
    return acc, correct, count


class EarlyStopping:
  def __init__(self, patience = 20, stop_epoch = 50, verbose = False):
    self.patience = patience
    self.stop_epoch = stop_epoch
    self.verbose = verbose
    self.counter = 0
    self.best_score = None
    self.early_stop = False
    self.val_loss_min = np.inf

  def __call__(self, epoch, val_loss, model, ckpt_name = 'checkpoint.pt'):
    score = -val_loss

    if self.best_score is None:
      self.best_score = score
      self.save_checkpoint(val_loss, model, ckpt_name)
    elif score < self.best_score:
      self.counter += 1
      if self.verbose: print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
      if self.counter >= self.patience and epoch > self.stop_epoch:
        self.early_stop = True
    else:
      self.best_score = score
      self.save_checkpoint(val_loss, model, ckpt_name)
      self.counter = 0

  def save_checkpoint(self, val_loss, model, ckpt_name):
    if self.verbose:
      print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')

    if hasattr(model, 'save_model'):
      model.save_model(ckpt_name)
    else:
      if self.verbose: print('Warning: You model did not provide a `save_model` function, we use torch.save to save you model...')
      torch.save(model.state_dict(), ckpt_name)
    
    self.val_loss_min = val_loss

[FILE-PATH: utils/data_loader.py]
[CODE]
import pandas as pd
import numpy as np
import os 
import h5py
import torch
from scipy import stats
from torch.utils.data import Dataset
from PIL import Image
from multiprocessing.pool import ThreadPool

from utils.clam_utils import generate_split, nth

class GenericWSIClassificationDataset(Dataset):
    def __init__(self,
      csv_path,
      label_column,
      label_dict = {},
      patient_voting = 'max',
      verbose = False,
    ):
      self.label_dict = label_dict
      self.num_classes = len(set(self.label_dict.values()))
      self.verbose = verbose
      self.train_ids, self.val_ids, self.test_ids  = (None, None, None)
      self.label_column = label_column
      self.seed = 1

      slide_data = pd.read_csv(csv_path, dtype=str)
      slide_data.drop(['data_provider'], axis=1, inplace=True)
      slide_data.rename(columns={'image_id': 'slide_id'}, inplace=True)

      slide_data['case_id'] = slide_data['slide_id']
      
      slide_data = self.dataframe_preparation(slide_data, self.label_dict, self.label_column)
      self.slide_data = slide_data

      self.patient_data_preparation(patient_voting)
      self.class_ids_preparation()

      if verbose:
        self.summarize()

      self.data_cache = {}

    def class_ids_preparation(self):
      # store ids corresponding each class at the patient or case level
      self.patient_cls_ids = [[] for i in range(self.num_classes)]		
      for i in range(self.num_classes):
        self.patient_cls_ids[i] = np.where(self.patient_data['label'] == i)[0]

      # store ids corresponding each class at the slide level
      self.slide_cls_ids = [[] for i in range(self.num_classes)]
      for i in range(self.num_classes):
        self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]

    def patient_data_preparation(self, patient_voting='max'):
      patients = np.unique(np.array(self.slide_data['case_id'])) # get unique patients
      patient_labels = []
      
      for p in patients:
        locations = self.slide_data[self.slide_data['case_id'] == p].index.tolist()
        assert len(locations) > 0
        label = self.slide_data['label'][locations].values
        if patient_voting == 'max': label = label.max() # get patient label (MIL convention)
        elif patient_voting == 'maj': label = stats.mode(label)[0]
        else: raise NotImplementedError
        patient_labels.append(label)
      
      self.patient_data = { 'case_id':patients, 'label': np.array(patient_labels) }

    @staticmethod
    def dataframe_preparation(data, label_dict, label_col):
      if label_col != 'label':
        data['label'] = data[label_col].copy()

      data.reset_index(drop=True, inplace=True)
      
      for i in data.index:
        key = data.loc[i, 'label']
        data.at[i, 'label'] = label_dict[key]

      return data

    def __len__(self):
      return len(self.slide_data)

    def summarize(self):
      print("label column: {}".format(self.label_column))
      print("label dictionary: {}".format(self.label_dict))
      print("number of classes: {}".format(self.num_classes))
      print("slide-level counts: ", '\n', self.slide_data['label'].value_counts(sort = False))
      for i in range(self.num_classes):
        print('Patient-LVL; Number of samples registered in class %d: %d' % (i, self.patient_cls_ids[i].shape[0]))
        print('Slide-LVL; Number of samples registered in class %d: %d' % (i, self.slide_cls_ids[i].shape[0]))

    def create_splits(
      self,
      k = 5,
      val_num = (25, 25),
      test_num = (40, 40),
    ):
      settings = {
        'n_splits' : k, 
        'val_num' : val_num, 
        'test_num': test_num,
        'seed': self.seed,
        'cls_ids': self.slide_cls_ids,
        'samples': len(self.slide_data)
      }

      self.split_gen = generate_split(**settings)

    def set_splits(self, start_from = None):
      if start_from: ids = nth(self.split_gen, start_from)
      else: ids = next(self.split_gen)
      self.train_ids, self.val_ids, self.test_ids = ids

    def get_split_from_df(self, backbone, patch_size, all_splits, split_key = 'train'):
      split = all_splits[split_key]
      split = split.dropna().reset_index(drop = True)

      if len(split) > 0:
        mask = self.slide_data['slide_id'].isin(split.tolist())
        df_slice = self.slide_data[mask].reset_index(drop = True)
        split = GenericSplit(df_slice, num_classes = self.num_classes)
        split.set_backbone(backbone)
        split.set_patch_size(patch_size)
      else: split = None
      
      return split

    def get_merged_split_from_df(self, all_splits, split_keys = ['train']):
      merged_split = []
      for split_key in split_keys:
        split = all_splits[split_key]
        split = split.dropna().reset_index(drop = True).tolist()
        merged_split.extend(split)

      if len(split) > 0:
        mask = self.slide_data['slide_id'].isin(merged_split)
        df_slice = self.slide_data[mask].reset_index(drop = True)
        split = GenericSplit(df_slice, num_classes = self.num_classes)
      else: split = None
      
      return split

    def return_splits(self, backbone = None, patch_size = '', from_id = True, csv_path = None):
      if from_id:
        if len(self.train_ids) > 0:
          train_data = self.slide_data.loc[self.train_ids].reset_index(drop = True)
          train_split = GenericSplit(train_data, num_classes = self.num_classes)
          train_split.set_backbone(backbone)
          train_split.set_patch_size(patch_size)
        else: train_split = None
        
        if len(self.val_ids) > 0:
          val_data = self.slide_data.loc[self.val_ids].reset_index(drop = True)
          val_split = GenericSplit(val_data, num_classes = self.num_classes)
          val_split.set_backbone(backbone)
          val_split.set_patch_size(patch_size)
        else: val_split = None
        
        if len(self.test_ids) > 0:
          test_data = self.slide_data.loc[self.test_ids].reset_index(drop = True)
          test_split = GenericSplit(test_data, num_classes = self.num_classes)
          test_split.set_backbone(backbone)
          test_split.set_patch_size(patch_size)
        else: test_split = None
      else:
        assert csv_path
        all_splits = pd.read_csv(csv_path, dtype=self.slide_data['slide_id'].dtype)
        train_split = self.get_split_from_df(backbone, patch_size, all_splits, 'train')
        val_split = self.get_split_from_df(backbone, patch_size, all_splits, 'val')
        test_split = self.get_split_from_df(backbone, patch_size, all_splits, 'test')
      
      return train_split, val_split, test_split

    def get_list(self, ids):
      return self.slide_data['slide_id'][ids]

    def getlabel(self, ids):
      return self.slide_data['label'][ids]

    def __getitem__(self, idx):
      return None

    def test_split_gen(self, return_descriptor = False, verbose = False):
      if return_descriptor:
        index = [list(self.label_dict.keys())[list(self.label_dict.values()).index(i)] for i in range(self.num_classes)]
        columns = ['train', 'val', 'test']
        df = pd.DataFrame(
          np.full((len(index), len(columns)), 0, dtype = np.int32), 
          index = index,
          columns = columns
        )

      count = len(self.train_ids)
      if verbose: print('\nnumber of training samples: {}'.format(count))
      
      labels = self.getlabel(self.train_ids)
      unique, counts = np.unique(labels, return_counts = True)
      
      for u in range(len(unique)):
        if verbose: print('number of samples in cls {}: {}'.format(unique[u], counts[u]))
        if return_descriptor: df.loc[index[u], 'train'] = counts[u]
      
      count = len(self.val_ids)
      if verbose: print('\nnumber of val samples: {}'.format(count))
      
      labels = self.getlabel(self.val_ids)
      unique, counts = np.unique(labels, return_counts = True)
      
      for u in range(len(unique)):
        if verbose: print('number of samples in cls {}: {}'.format(unique[u], counts[u]))
        if return_descriptor: df.loc[index[u], 'val'] = counts[u]

      count = len(self.test_ids)
      if verbose: print('\nnumber of test samples: {}'.format(count))
      
      labels = self.getlabel(self.test_ids)
      unique, counts = np.unique(labels, return_counts = True)
      
      for u in range(len(unique)):
        if verbose: print('number of samples in cls {}: {}'.format(unique[u], counts[u]))
        if return_descriptor: df.loc[index[u], 'test'] = counts[u]

      assert len(np.intersect1d(self.train_ids, self.test_ids)) == 0
      assert len(np.intersect1d(self.train_ids, self.val_ids)) == 0
      assert len(np.intersect1d(self.val_ids, self.test_ids)) == 0

      if return_descriptor: return df

    def save_split(self, filename):
      train_split = self.get_list(self.train_ids)
      val_split = self.get_list(self.val_ids)
      test_split = self.get_list(self.test_ids)
      df_tr = pd.DataFrame({'train': train_split})
      df_v = pd.DataFrame({'val': val_split})
      df_t = pd.DataFrame({'test': test_split})
      df = pd.concat([df_tr, df_v, df_t], axis=1) 
      df.to_csv(filename, index = False)


class GenericMILDataset(GenericWSIClassificationDataset):
  def __init__(
    self,
    extract_patches_dir = None, 
    patches_dir = None,
    features_pt_directory = None,
    **kwargs
  ):
    super(GenericMILDataset, self).__init__(**kwargs)

    self.extract_patches_dir = extract_patches_dir
    self.patches_dir = patches_dir
    self.features_pt_directory = features_pt_directory

  def __getitem__(self, index):
    slide_id = self.slide_data['slide_id'][index]
    label = self.slide_data['label'][index]

    is_google_colab = os.getcwd().lower() == '/content'
    if not hasattr(self, 'extract_patches_dir'):
      if is_google_colab: self.extract_patches_dir = "/content/output/extract_patches"
      else: self.extract_patches_dir = "/kaggle/working/output/extract_patches"
    if not hasattr(self, 'patches_dir'):
      if is_google_colab: self.patches_dir = "/content/output/create_patches/patches"
      else: self.patches_dir = "/kaggle/working/output/create_patches/patches"
    if not hasattr(self, 'features_pt_directory'):
      if is_google_colab: self.features_pt_directory = "/content/output/extract_features/pt_files"
      else: self.features_pt_directory = "/kaggle/working/output/extract_features/pt_files"
      
    img_root = os.path.join(self.extract_patches_dir, slide_id)
    coords_path = os.path.join(self.patches_dir, '{}.h5'.format(slide_id))

    try:
      with h5py.File(coords_path, 'r') as f:
        coords = f['coords'][:]
    except:
      pass

    def image_caller(coor_index):
      img_list = []
      for c in coor_index:
        _x, _y = coords[c]
        path = os.path.join(img_root, f'{_x}_{_y}_{self.patch_size}_{self.patch_size}.jpg')
        img = Image.open(path).convert('RGB')
        img_list.append(img)
      return img_list
    
    full_path = os.path.join(self.features_pt_directory, self.backbone, '{}.pt'.format(slide_id))
    try:
      features = torch.load(full_path)
    except:
      raise RuntimeError('failed to load:{}'.format(full_path))

    label = torch.LongTensor([label])
    output = {
      'features': features, 'label': label,
      'image_call': image_caller,
      'coords': coords,
    }
    return output

  def set_backbone(self, backbone):
    self.backbone = backbone
  
  def set_patch_size(self, size):
    self.patch_size = size

class GenericSplit(GenericMILDataset):
    def __init__(self, slide_data, num_classes = 2):      
      self.use_h5 = False
      self.slide_data = slide_data
      self.num_classes = num_classes
      self.slide_cls_ids = [[] for i in range(self.num_classes)]
      self.data_cache = {}
      for i in range(self.num_classes):
        self.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]

    def __len__(self):
      return len(self.slide_data)
        
    def set_backbone(self, backbone):
      self.backbone = backbone

    def set_patch_size(self, size):
      self.patch_size = size

    def pre_loading(self, thread = os.cpu_count()):
      self.cache_flag = True
      ids = list(range(len(self)))
      exe = ThreadPool(thread)
      exe.map(self.__getitem__, ids)

[FILE-PATH: utils/file_utils.py]
[CODE]
import json
import pickle
import h5py

def save_pkl(
  filename,
  save_object
):
	writer = open(filename,'wb')
	pickle.dump(save_object, writer)
	writer.close()

def load_pkl(filename):
	loader = open(filename,'rb')
	file = pickle.load(loader)
	loader.close()
	return file

def save_hdf5(
  output_path,
  asset_dict,
  attr_dict = None, 
  mode='a'
):
  file = h5py.File(output_path, mode)
  
  for key, val in asset_dict.items():
      data_shape = val.shape
      
      if key not in file:
        data_type = val.dtype
        chunk_shape = (1, ) + data_shape[1:]
        maxshape = (None, ) + data_shape[1:]
        
        dset = file.create_dataset(
          key,
          shape = data_shape,
          maxshape = maxshape,
          chunks = chunk_shape,
          dtype = data_type
        )
        dset[:] = val
        
        if attr_dict is not None:
          if key in attr_dict.keys():
            for attr_key, attr_val in attr_dict[key].items():
              dset.attrs[attr_key] = attr_val
      else:
        dset = file[key]
        dset.resize(len(dset) + data_shape[0], axis=0)
        dset[-data_shape[0]:] = val

  file.close()
  return output_path

def save_json(
  output_path,
  asset_dict
):
  with open(output_path, 'w') as f:
    json.dump(asset_dict, f)

[FILE-PATH: utils/helper.py]
[CODE]
import os

def create_directories(directories: dict) -> None:
  """
  Creates directories from a dictionary of paths if they don't exist.
  
  Args:
      directories (dict): Dictionary containing directory paths
  """
  for directory in directories.values():
    if not os.path.exists(directory):
      os.makedirs(directory)

[FILE-PATH: utils/logger.py]
[CODE]
import os
import datetime
from colorama import Fore, Style, init
from tqdm import tqdm

init(autoreset=True)

class Logger:
  COLORS = {
    'INFO': Fore.CYAN,
    'SUCCESS': Fore.GREEN,
    'WARNING': Fore.YELLOW,
    'ERROR': Fore.RED,
    'DEBUG': Fore.MAGENTA,
  }
  
  def __init__(self):
    pass
  
  def _get_timestamp(self):
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
  
  def _log(self, level, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    if args or kwargs:
      message = message.format(*args, **kwargs)
    
    color = self.COLORS.get(level, Fore.WHITE)
    
    if timestamp:
      time_str = self._get_timestamp()
      prefix = f"{color}[{time_str}] [{level}]"
    else:
      prefix = f"{color}[{level}]"
    
    formatted_message = f"{prefix} {message}{Style.RESET_ALL}"
    
    if use_tqdm:
      tqdm.write(formatted_message)
    else:
      print(formatted_message)
  
  def text(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    if args or kwargs: 
      message = message.format(*args, **kwargs)
    
    if timestamp:
      time_str = self._get_timestamp()
      formatted_message = f"[{time_str}] {message}"
    else:
      formatted_message = f"{message}"
    
    if use_tqdm:
      tqdm.write(formatted_message)
    else:
      print(formatted_message)
  
  def info(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    self._log("INFO", message, *args, timestamp=timestamp, use_tqdm=use_tqdm, **kwargs)
  
  def success(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    self._log("SUCCESS", message, *args, timestamp=timestamp, use_tqdm=use_tqdm, **kwargs)
  
  def warning(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    self._log("WARNING", message, *args, timestamp=timestamp, use_tqdm=use_tqdm, **kwargs)
  
  def error(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    self._log("ERROR", message, *args, timestamp=timestamp, use_tqdm=use_tqdm, **kwargs)
  
  def debug(self, message, *args, timestamp=False, use_tqdm=True, **kwargs):
    self._log("DEBUG", message, *args, timestamp=timestamp, use_tqdm=use_tqdm, **kwargs)

  def draw_header(
    self, 
    heading,
    version = "1",
    width = 50,
    clear_screen = True
  ):
    if clear_screen:
      self.clear_screen()

    border_color = Fore.WHITE
    heading_color = Fore.WHITE
    content_color = Fore.WHITE
    project_color = Fore.WHITE
    
    borders = {
      'tl': '┌', 'tr': '┐', 'bl': '└', 'br': '┘',
      'h': '─', 'v': '│', 'lj': '├', 'rj': '┤'
    }
    
    def format_border(left, middle, right):
      return f"{border_color}{left}{middle * (width-2)}{right}{Style.RESET_ALL}"
    
    def format_line(text, color, centered=True, padding_char=' '):
      max_len = width - 4
      if len(text) > max_len:
        return wrap_text(text, color, centered)
      
      if centered:
        padding = max(0, width - len(text) - 4)
        left_pad = padding // 2
        right_pad = padding - left_pad
        padding_str = f"{padding_char * left_pad}{text}{padding_char * right_pad}"
      else:
        padding_str = f"{text}{padding_char * (max_len - len(text))}"
      
      return [f"{border_color}{borders['v']}{Style.RESET_ALL} {color}{padding_str}{Style.RESET_ALL} {border_color}{borders['v']}{Style.RESET_ALL}"]
    
    def wrap_text(text, color, centered=True):
      lines = []
      max_text_length = width - 4
      
      if len(text) <= max_text_length:
        return format_line(text, color, centered)
      
      words = text.split()
      current_line = ""
      for word in words:
        if len(current_line) + len(word) + 1 <= max_text_length:
          current_line = word if not current_line else f"{current_line} {word}"
        else:
          lines.extend(format_line(current_line, color, centered))
          current_line = word
      
      if current_line:
        lines.extend(format_line(current_line, color, centered))
      
      return lines
    
    top_border = format_border(borders['tl'], borders['h'], borders['tr'])
    bottom_border = format_border(borders['bl'], borders['h'], borders['br'])
    separator = format_border(borders['lj'], borders['h'], borders['rj'])
    
    output_lines = [top_border]
    
    # header title
    heading_with_version = f"{heading} (v{version})"
    output_lines.extend(wrap_text(heading_with_version, heading_color))
    output_lines.append(separator)

    # content lines
    content_items = [
      f"Dilawaiz Sarwat",
      f"» 23015919-007@uog.edu.pk",
      f"» MPhil Computer Science",
      f"» Department of Computer Science",
      f"» University of Gujrat, Hafiz Hayat Campus"
    ]
    
    for item in content_items:
      output_lines.extend(format_line(item, content_color, centered=False))
    
    output_lines.append(separator)
    
    # project title
    project_title = "A Deep Learning Framework for Prostate Cancer Analysis with Weak Supervision"
    output_lines.extend(wrap_text(project_title, project_color))
    
    output_lines.append(bottom_border)
    
    for line in output_lines: print(line)
    self.empty_line()

  def empty_line(self, use_tqdm=True):
    if use_tqdm:
      tqdm.write("")
    else:
      print()

  def clear_screen(self):
    os.system('cls' if os.name=='nt' else 'clear')

[FILE-PATH: utils/train_engine.py]
[CODE]
import torch
import os
import numpy as np
import pandas as pd
from torch.utils.tensorboard.writer import SummaryWriter
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score, roc_curve, f1_score
from sklearn.metrics import auc as calc_auc
from sklearn.metrics import precision_score, recall_score, cohen_kappa_score, confusion_matrix, classification_report, accuracy_score
from torch.utils.tensorboard.writer import SummaryWriter
from tqdm import tqdm

from mil_models import find_mil_model
from utils.common_utils import EarlyStopping, AccuracyLogger
from utils.clam_utils import print_network, get_split_loader, calculate_error
from utils.logger import Logger

def save_splits(
  dataset_splits,
  column_keys,
  filename,
  boolean_style = False
):
  splits = [dataset_splits[i].slide_data['slide_id'] for i in range(len(dataset_splits))]
  if not boolean_style:
    df = pd.concat(splits, ignore_index = True, axis = 1)
    df.columns = column_keys
  else:
    df = pd.concat(splits, ignore_index = True, axis = 0)
    one_hot = np.eye(len(dataset_splits)).astype(bool)
    bool_array = np.repeat(one_hot, [len(dset) for dset in dataset_splits], axis = 0)
    df = pd.DataFrame(
      bool_array,
      index = df.values.tolist(), 
      columns = ['train', 'val', 'test']
    )

  df.to_csv(filename)

class TrainEngine:
  def __init__(
    self,
    datasets,
    fold,
    result_directory,
    mil_model_name,
    learning_rate,
    max_epochs,
    in_dim,
    n_classes,
    drop_out,
    weighted_sample = False,
    optimizer_name = 'adam',
    regularization = 1e-5,
    batch_size = 1,
    bag_loss = 'ce',
    verbose = False
  ):
    self.train_split, self.val_split, self.test_split = datasets
    self.fold = fold
    self.result_dir = result_directory
    self.mil_model_name = mil_model_name
    self.optimizer_name = optimizer_name
    self.learning_rate = learning_rate
    self.regularization = regularization
    self.weighted_sample = weighted_sample
    self.batch_size = batch_size
    self.max_epochs = max_epochs
    self.in_dim = in_dim
    self.n_classes = n_classes
    self.drop_out = drop_out    
    self.bag_loss = bag_loss
    self.verbose = verbose
    self.logger = Logger()

    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    save_splits(
      datasets,
      ['train', 'val', 'test'],
      os.path.join(self.result_dir, 'splits_{}.csv'.format(fold))
    )

    self.init_logger()

    self.call_scheduler = self.get_learning_rate_scheduler()
    self.model = self.get_mil_model()
    self.loss_function = self.get_loss_function()
    self.optimizer = self.get_optimizer()
    
    self.early_stopping = EarlyStopping(
      patience = self.model.early_stopping_patience if hasattr(self.model, 'early_stopping_patience') else 20,
      stop_epoch = self.model.early_stopping_stop_epoch if hasattr(self.model, 'early_stopping_stop_epoch') else 50,
      verbose = self.verbose
    )

    self.train_loader, self.val_loader, self.test_loader = self.init_data_loaders()

    # core, if you implement your training framework, call setup to pass training hyperparameters.
    if hasattr(self.model, 'set_up'):
      extra_args = { 'total_iterations': max_epochs * len(self.train_loader) }
      self.model.set_up(
        lr = learning_rate,
        max_epochs = max_epochs,
        weight_decay = regularization,
        **extra_args
      )
      
  def init_logger(self):
    self.logger.info('Training Fold {}!'.format(self.fold), timestamp=True)
    
    writer_dir = os.path.join(self.result_dir, str(self.fold))
    if not os.path.isdir(writer_dir): os.mkdir(writer_dir)
    self.writer = SummaryWriter(writer_dir, flush_secs = 15)

    self.logger.info("Training on {} samples".format(len(self.train_split)))
    self.logger.info("Validating on {} samples".format(len(self.val_split)))
    self.logger.info("Testing on {} samples".format(len(self.test_split)))
    self.logger.info(f"Using device: {self.device}")

  def get_learning_rate_scheduler(self):
    return None

  def get_mil_model(self):
    model = find_mil_model(
      self.mil_model_name,
      self.in_dim,
      self.n_classes,
      self.drop_out
    )

    if hasattr(model, 'relocate'): model.relocate()
    else: model = model.to(self.device)

    if self.verbose:
      print_network(model)
    else:
      self.logger.info(f"Model: {self.mil_model_name} initialized")
    return model

  def get_loss_function(self):
    if hasattr(self.model, 'loss_function'):
      if self.verbose:
        self.logger.info('The loss function defined in the MIL model is adopted...')
      loss_function = self.model.loss_function
    else:
      if self.verbose:
        self.logger.info('Cross Entropy Loss is adopted as the loss function...')
      loss_function = torch.nn.CrossEntropyLoss()
    return loss_function

  def get_optimizer(self):
    if self.optimizer_name.lower() == "adam":
      optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, self.model.parameters()),
        lr = self.learning_rate,
        weight_decay = self.regularization
      )
    elif self.optimizer_name.lower() == 'sgd':
      optimizer = torch.optim.SGD(
        filter(lambda p: p.requires_grad, self.model.parameters()),
        lr = self.learning_rate,
        momentum = 0.9,
        weight_decay = self.regularization
      )
    else:
      raise NotImplementedError(f'Optimizer {self.optimizer_name} not implemented!')
    
    self.logger.info(f"Using {self.optimizer_name} optimizer with lr={self.learning_rate}")
    return optimizer

  def init_data_loaders(self):
    train_loader = get_split_loader(
      self.train_split,
      training = True,
      weighted = self.weighted_sample,
      batch_size = self.batch_size
    )
    val_loader = get_split_loader(self.val_split)
    test_loader = get_split_loader(self.test_split)
    return train_loader, val_loader, test_loader

  def train_model(self, fold):
    train_loop_func = self.train_loop_subtyping
    validate_func = self.validate_subtyping
    test_func = self.summary_subtyping

    self.logger.info("Starting training process", timestamp=True)
    for epoch in range(self.max_epochs):
      train_loop_func(epoch)
      stop = validate_func(epoch)
      if stop: 
        self.logger.warning("Early stopping triggered", timestamp=True)
        break
    
    checkpoint_path = os.path.join(self.result_dir, "s_{}_checkpoint.pt".format(fold))
    msg = self.model.load_state_dict(torch.load(checkpoint_path))
    
    self.logger.info('Loading best model checkpoint from: {}'.format(checkpoint_path), timestamp=True)
    if self.verbose:
      self.logger.debug(msg)
    
    # test_func on val loader
    self.logger.info("Evaluating on validation set...", timestamp=True)
    _, val_error, val_auc, _, _, val_f1, val_metrics = test_func(self.val_loader)
    
    # test on test loader
    self.logger.info("Evaluating on test set...", timestamp=True)
    results_dict, test_error, test_auc, acc_logger, df, test_f1, test_metrics = test_func(self.test_loader)
    
    self.logger.success("Final Results:", timestamp=True)
    self.logger.success("Test Error: {:.4f}, ROC AUC: {:.4f}, F1 Score: {:.4f}".format(test_error, test_auc, test_f1))
    self.logger.success("Val Error: {:.4f}, ROC AUC: {:.4f}, F1 Score: {:.4f}".format(val_error, val_auc, val_f1))

    for i in range(self.n_classes):
      acc, correct, count = acc_logger.get_summary(i)
      self.logger.info('Class {}: Accuracy {:.4f}, Correct {}/{}'.format(i, acc, correct, count))
      self.writer.add_scalar('final/test_class_{}_acc'.format(i), acc, 0)

    self.writer.add_scalar('final/val_error', val_error, 0)
    self.writer.add_scalar('final/val_auc', val_auc, 0)
    self.writer.add_scalar('final/test_error', test_error, 0)
    self.writer.add_scalar('final/test_auc', test_auc, 0)
    self.writer.add_scalar('final/val_precision', val_metrics['precision_macro'], 0)
    self.writer.add_scalar('final/test_precision', test_metrics['precision_macro'], 0)
    self.writer.add_scalar('final/val_recall', val_metrics['recall_macro'], 0)
    self.writer.add_scalar('final/test_recall', test_metrics['recall_macro'], 0)
    self.writer.add_scalar('final/val_kappa', val_metrics['cohens_kappa'], 0)
    self.writer.add_scalar('final/test_kappa', test_metrics['cohens_kappa'], 0)
    self.writer.close()
    
    return results_dict, test_auc, val_auc, 1-test_error, 1-val_error, test_f1, val_f1, test_metrics, val_metrics

  def train_loop_subtyping(self, epoch):   
    self.model.train()
    acc_logger = AccuracyLogger(n_classes = self.n_classes)
    train_loss = 0.0
    train_error = 0.0

    self.logger.empty_line()
    self.logger.info('Epoch: {}/{}'.format(epoch+1, self.max_epochs), timestamp=True)
    
    # Store debug messages to print after batch completion
    debug_messages = []
    
    progress_bar = tqdm(
        self.train_loader, 
        desc=f"Training Epoch {epoch+1}/{self.max_epochs}",
        disable=False, 
        ncols=100,
        leave=True  # Ensure progress bar is left on screen after completion
    )
    
    for batch_idx, batch in enumerate(progress_bar):
      iteration = epoch * len(self.train_loader) + batch_idx
      kwargs = {}
      data = batch['features']
      label = batch['label']
      kwargs['iteration'] = iteration
      kwargs['image_call'] = batch['image_call']
      
      # Core 1: if your model need specific pre-process of data and label
      if hasattr(self.model, 'process_data'):
        data, label = self.model.process_data(data, label, self.device)
      else:
        data = data.to(self.device)
        label = label.to(self.device)
        
      # Core 2: Model optimization step
      if hasattr(self.model, 'one_step'):
        outputs = self.model.one_step(data, label, **kwargs)
        loss = outputs['loss']
        if 'call_scheduler' in outputs.keys():
          self.call_scheduler = outputs['call_scheduler']
        logits, Y_prob, Y_hat = outputs['wsi_logits'], outputs['wsi_prob'], outputs['wsi_label']
      else:
        # use universal code to update param
        kwargs['label'] = label
        outputs = self.model(data, **kwargs)
        logits, Y_prob, Y_hat = outputs['wsi_logits'], outputs['wsi_prob'], outputs['wsi_label']

        if hasattr(self.model, 'loss_function'): loss = self.loss_function(logits, label, **outputs)
        else: loss = self.loss_function(logits, label)

        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        outputs['loss'] = loss
      
      # to support batch size greater than 1
      if isinstance(label, torch.Tensor):
        acc_logger.log(Y_hat, label)
      else:
        for i in range(len(data)):
          acc_logger.log(Y_hat[i], label[i])

      loss_value = loss.item()
      if torch.isnan(loss):
        progress_bar.close()  # Close the progress bar before showing error
        self.logger.error('NaN loss detected!')
        if self.verbose:
          self.logger.debug('logits: {}'.format(logits))
          self.logger.debug('Y_prob: {}'.format(Y_prob))
          self.logger.debug('loss: {}'.format(loss))
        raise RuntimeError('Found Nan number')
      
      # Update progress bar
      error = calculate_error(Y_hat, label)
      progress_bar.set_postfix({
          'loss': f'{loss_value:.4f}',
          'error': f'{error:.4f}'
      })
      
      # Instead of printing debug messages directly, store them for later
      if (batch_idx + 1) % 20 == 0 and self.verbose:
        bag_size = data[0].shape[0] if isinstance(data, list) else data.shape[0]
        log_message = f'Batch {batch_idx+1}/{len(self.train_loader)}'
        for k, v in outputs.items():
          if 'loss' in k:
            log_message += f', {k}: {v.item():.4f}'
        log_message += f', label: {label.item()}, bag_size: {bag_size}'
        
        # Use tqdm.write to print without breaking the progress bar
        tqdm.write(f"[DEBUG] {log_message}")
              
      train_loss += loss_value
      train_error += error
    
    # Close progress bar properly
    progress_bar.close()

    # calculate loss and error for epoch
    train_loss /= len(self.train_loader)
    train_error /= len(self.train_loader)
    train_acc = 1 - train_error

    self.logger.success('Epoch: {}/{}, Train Loss: {:.4f}, Train Error: {:.4f}, Train Accuracy: {:.4f}'.format(
        epoch+1, self.max_epochs, train_loss, train_error, train_acc))
    
    if self.writer:
      self.writer.add_scalar('train/loss', train_loss, epoch)
      self.writer.add_scalar('train/error', train_error, epoch)
      self.writer.add_scalar('train/accuracy', train_acc, epoch)
    
    # Class-specific metrics  
    if self.verbose:
      for i in range(self.n_classes):
        acc, correct, count = acc_logger.get_summary(i)
        self.logger.info('Class {}: Accuracy {:.4f}, Correct {}/{}'.format(i, acc, correct, count))
        if self.writer:
          self.writer.add_scalar('train/class_{}_acc'.format(i), acc, epoch)

    if self.call_scheduler is not None:
      self.call_scheduler()

  def validate_subtyping(self, epoch):
    self.model.eval()
    acc_logger = AccuracyLogger(n_classes = self.n_classes)
    val_loss = 0.0
    val_error = 0.0

    prob = np.zeros((len(self.val_loader), self.n_classes))
    labels = np.zeros(len(self.val_loader))    
    Y_hats = np.zeros(len(self.val_loader))    
    
    # Lists to store predictions and labels for comprehensive metrics
    all_Y_hat = []
    all_label = []
    
    self.logger.info("Validating epoch {}/{}".format(epoch+1, self.max_epochs), timestamp=True)
    
    progress_bar = tqdm(
        self.val_loader, 
        desc=f"Validating Epoch {epoch+1}/{self.max_epochs}", 
        disable=False,
        ncols=100,
        leave=True  # Ensure progress bar is left on screen
    )
    
    with torch.no_grad():
      for batch_idx, batch in enumerate(progress_bar):
        kwargs = {}
        data = batch['features']
        label = batch['label']
        kwargs['image_call'] = batch['image_call']
        
        if hasattr(self.model, 'process_data'):
          data, label = self.model.process_data(data, label, self.device)
        else:
          data = data.to(self.device)
          label = label.to(self.device)

        if hasattr(self.model, 'wsi_predict'):
          outputs = self.model.wsi_predict(data, **kwargs)
        else: # use universal code to update param
          outputs = self.model(data)

        logits, Y_prob, Y_hat = outputs['wsi_logits'], outputs['wsi_prob'], outputs['wsi_label']
        acc_logger.log(Y_hat, label)
        try: loss = self.loss_function(logits, label, **outputs)
        except: loss = self.loss_function(logits, label)
            
        prob[batch_idx] = Y_prob.cpu().numpy()
        labels[batch_idx] = label.item()
        Y_hats[batch_idx] = Y_hat.item()
        
        # Append current predictions and labels to the lists
        all_Y_hat.append(Y_hat.cpu().numpy())
        all_label.append(label.cpu().numpy())
        
        loss_value = loss.item()
        error = calculate_error(Y_hat, label)
        
        # Update progress bar
        progress_bar.set_postfix({
            'loss': f'{loss_value:.4f}',
            'error': f'{error:.4f}'
        })
        
        val_loss += loss_value
        val_error += error
    
    # Close progress bar properly
    progress_bar.close()

    val_error /= len(self.val_loader)
    val_loss /= len(self.val_loader)

    # Convert the lists of all predictions and labels to numpy arrays
    all_Y_hat = np.concatenate(all_Y_hat)
    all_label = np.concatenate(all_label)
    
    # Calculate all metrics for the epoch
    metrics = {}
    
    # Accuracy from error
    metrics['accuracy'] = 1 - val_error
    
    # ROC AUC
    if self.n_classes == 2:
      auc = roc_auc_score(labels, prob[:, 1])
      aucs = []
      metrics['auc'] = auc
    else:
      aucs = []
      binary_labels = label_binarize(labels, classes=[i for i in range(self.n_classes)])
      for class_idx in range(self.n_classes):
        if class_idx in labels:
          fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], prob[:, class_idx])
          aucs.append(calc_auc(fpr, tpr))
        else:
          aucs.append(float('nan'))

      auc = np.nanmean(np.array(aucs))
      metrics['auc'] = auc
      metrics['class_auc'] = aucs
    
    # F1 Score (macro)
    f1 = f1_score(all_label, all_Y_hat, average='macro')
    metrics['f1_macro'] = f1
    
    # F1 Score (weighted)
    f1_weighted = f1_score(all_label, all_Y_hat, average='weighted')
    metrics['f1_weighted'] = f1_weighted
    
    # Class-wise F1 Score
    f1_per_class = f1_score(all_label, all_Y_hat, average=None)
    metrics['f1_per_class'] = f1_per_class
    
    # Precision (macro and weighted)
    precision_macro = precision_score(all_label, all_Y_hat, average='macro')
    precision_weighted = precision_score(all_label, all_Y_hat, average='weighted')
    metrics['precision_macro'] = precision_macro
    metrics['precision_weighted'] = precision_weighted
    
    # Recall (macro and weighted)
    recall_macro = recall_score(all_label, all_Y_hat, average='macro')
    recall_weighted = recall_score(all_label, all_Y_hat, average='weighted')
    metrics['recall_macro'] = recall_macro
    metrics['recall_weighted'] = recall_weighted
    
    # Cohen's Kappa
    kappa = cohen_kappa_score(all_label, all_Y_hat)
    metrics['cohens_kappa'] = kappa
    
    # Add metrics to TensorBoard
    if self.writer:
      self.writer.add_scalar('val/loss', val_loss, epoch)
      self.writer.add_scalar('val/auc', auc, epoch)
      self.writer.add_scalar('val/error', val_error, epoch)
      self.writer.add_scalar('val/accuracy', metrics['accuracy'], epoch)
      self.writer.add_scalar('val/f1_macro', metrics['f1_macro'], epoch)
      self.writer.add_scalar('val/f1_weighted', metrics['f1_weighted'], epoch)
      self.writer.add_scalar('val/precision_macro', metrics['precision_macro'], epoch)
      self.writer.add_scalar('val/precision_weighted', metrics['precision_weighted'], epoch)
      self.writer.add_scalar('val/recall_macro', metrics['recall_macro'], epoch)
      self.writer.add_scalar('val/recall_weighted', metrics['recall_weighted'], epoch)
      self.writer.add_scalar('val/cohens_kappa', metrics['cohens_kappa'], epoch)

    # Log comprehensive metrics for this epoch
    self.logger.success('Validation Results - Epoch: {}/{}'.format(epoch+1, self.max_epochs), timestamp=True)
    self.logger.info('  Loss: {:.4f}, Error: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_error, metrics['accuracy']))
    self.logger.info('  AUC: {:.4f}, F1 (macro): {:.4f}, F1 (weighted): {:.4f}'.format(
        auc, metrics['f1_macro'], metrics['f1_weighted']))
    self.logger.info('  Precision (macro): {:.4f}, Recall (macro): {:.4f}'.format(
        metrics['precision_macro'], metrics['recall_macro']))
    self.logger.info('  Cohen\'s Kappa: {:.4f}'.format(metrics['cohens_kappa']))
    
    # Class-specific metrics if in verbose mode
    if self.verbose:
      self.logger.info('Class-specific metrics:')
      for i in range(self.n_classes):
        acc, correct, count = acc_logger.get_summary(i)
        self.logger.info('  Class {}: Acc {:.4f}, F1 {:.4f}, Precision {:.4f}, Recall {:.4f}'.format(
            i, acc, 
            f1_per_class[i] if i < len(f1_per_class) else float('nan'),
            metrics['precision_per_class'][i] if i < len(metrics['precision_per_class']) else float('nan'),
            metrics['recall_per_class'][i] if i < len(metrics['recall_per_class']) else float('nan')))
        
        if self.writer:
          self.writer.add_scalar('val/class_{}_acc'.format(i), acc, epoch)
          if i < len(f1_per_class):
            self.writer.add_scalar('val/class_{}_f1'.format(i), f1_per_class[i], epoch)
          if i < len(metrics['precision_per_class']):
            self.writer.add_scalar('val/class_{}_precision'.format(i), metrics['precision_per_class'][i], epoch)
          if i < len(metrics['recall_per_class']):
            self.writer.add_scalar('val/class_{}_recall'.format(i), metrics['recall_per_class'][i], epoch)

    # val_error is better than val_loss
    self.early_stopping(epoch, val_error, self.model, ckpt_name = os.path.join(self.result_dir, "s_{}_checkpoint.pt".format(self.fold)))
    
    if self.early_stopping.early_stop:
      self.logger.warning("Early stopping triggered at epoch {}/{}".format(epoch+1, self.max_epochs))
      return True
    else:
      return False

  def summary_subtyping(self, loader = None):
    if loader is None:
      loader = self.test_loader
    
    acc_logger = AccuracyLogger(n_classes = self.n_classes)
    self.model.eval()
    test_error = 0.0

    all_probs = np.zeros((len(loader), self.n_classes))
    all_labels = np.zeros(len(loader))
    all_preds = np.zeros(len(loader))

    slide_ids = loader.dataset.slide_data['slide_id']
    patient_results = {}
    
    # create lists to store all predictions and labels
    all_Y_hat = []
    all_label = []
    
    self.logger.info("Running evaluation...", timestamp=True)
    
    progress_bar = tqdm(
        loader, 
        desc="Evaluating", 
        disable=False,
        ncols=100,
        leave=True  # Ensure progress bar is left on screen
    )
    
    with torch.no_grad():
      for batch_idx, batch in enumerate(progress_bar):
        data = batch['features']
        label = batch['label']
        
        if hasattr(self.model, 'process_data'):
          data, label = self.model.process_data(data, label, self.device)
        else:
          data = data.to(self.device)
          label = label.to(self.device)

        if hasattr(self.model, 'wsi_predict'):
          outputs = self.model.wsi_predict(data, **batch)
        else: # use universal code to update param
          outputs = self.model(data)

        logits, Y_prob, Y_hat = outputs['wsi_logits'], outputs['wsi_prob'], outputs['wsi_label']
        slide_id = slide_ids.iloc[batch_idx]
        acc_logger.log(Y_hat, label)

        probs = Y_prob.cpu().numpy()
        all_probs[batch_idx] = probs
        all_labels[batch_idx] = label.item()
        all_preds[batch_idx] = Y_hat.item()
        
        patient_results.update({slide_id: {'slide_id': np.array(slide_id), 'prob': probs, 'label': label.item()}})
        error = calculate_error(Y_hat, label)
        test_error += error
        
        # Update progress bar
        progress_bar.set_postfix({
            'error': f'{error:.4f}'
        })
        
        # Append current predictions and labels to the lists
        all_Y_hat.append(Y_hat.cpu().numpy())
        all_label.append(label.cpu().numpy())
    
    # Close progress bar properly
    progress_bar.close()

    test_error /= len(loader)

    # convert the lists of all predictions and labels to numpy arrays
    all_Y_hat = np.concatenate(all_Y_hat)
    all_label = np.concatenate(all_label)
    
    # Calculate comprehensive metrics
    metrics = {}
    
    # Accuracy from error
    metrics['accuracy'] = 1 - test_error
    
    # ROC AUC
    if self.n_classes == 2:
      auc = roc_auc_score(all_labels, all_probs[:, 1])
      aucs = []
      metrics['auc'] = auc
    else:
      aucs = []
      binary_labels = label_binarize(all_labels, classes=[i for i in range(self.n_classes)])
      for class_idx in range(self.n_classes):
        if class_idx in all_labels:
          fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], all_probs[:, class_idx])
          aucs.append(calc_auc(fpr, tpr))
        else:
          aucs.append(float('nan'))

      auc = np.nanmean(np.array(aucs))
      metrics['auc'] = auc
      metrics['class_auc'] = aucs
    
    # F1 Score (macro)
    f1 = f1_score(all_label, all_Y_hat, average='macro')
    metrics['f1_macro'] = f1
    
    # F1 Score (weighted)
    f1_weighted = f1_score(all_label, all_Y_hat, average='weighted')
    metrics['f1_weighted'] = f1_weighted
    
    # Class-wise F1 Score
    f1_per_class = f1_score(all_label, all_Y_hat, average=None)
    metrics['f1_per_class'] = f1_per_class
    
    # Precision (macro and weighted)
    precision_macro = precision_score(all_label, all_Y_hat, average='macro')
    precision_weighted = precision_score(all_label, all_Y_hat, average='weighted')
    metrics['precision_macro'] = precision_macro
    metrics['precision_weighted'] = precision_weighted
    
    # Class-wise precision
    precision_per_class = precision_score(all_label, all_Y_hat, average=None)
    metrics['precision_per_class'] = precision_per_class
    
    # Recall (macro and weighted)
    recall_macro = recall_score(all_label, all_Y_hat, average='macro')
    recall_weighted = recall_score(all_label, all_Y_hat, average='weighted')
    metrics['recall_macro'] = recall_macro
    metrics['recall_weighted'] = recall_weighted
    
    # Class-wise recall
    recall_per_class = recall_score(all_label, all_Y_hat, average=None)
    metrics['recall_per_class'] = recall_per_class
    
    # Cohen's Kappa
    kappa = cohen_kappa_score(all_label, all_Y_hat)
    metrics['cohens_kappa'] = kappa
    
    # Confusion Matrix
    cm = confusion_matrix(all_label, all_Y_hat)
    metrics['confusion_matrix'] = cm
    
    # Classification Report (for logging purposes)
    cr = classification_report(all_label, all_Y_hat, output_dict=True)
    metrics['classification_report'] = cr

    # Log major metrics
    self.logger.success(f"Evaluation Results:", timestamp=True)
    self.logger.success(f"  Error: {test_error:.4f}")
    self.logger.success(f"  Accuracy: {metrics['accuracy']:.4f}")
    self.logger.success(f"  AUC: {auc:.4f}")
    self.logger.success(f"  F1 Score (macro): {f1:.4f}")
    self.logger.success(f"  Precision (macro): {precision_macro:.4f}")
    self.logger.success(f"  Recall (macro): {recall_macro:.4f}")
    self.logger.success(f"  Cohen's Kappa: {kappa:.4f}")
    
    # Log confusion matrix
    self.logger.info("Confusion Matrix:")
    self.logger.info(f"\n{cm}")
    
    if self.verbose:
      for i in range(self.n_classes):
        acc, correct, count = acc_logger.get_summary(i)
        self.logger.info('Class {}: Accuracy {:.4f}, Correct {}/{}'.format(i, acc, correct, count))
        self.logger.info(f'  Precision: {precision_per_class[i]:.4f}, Recall: {recall_per_class[i]:.4f}, F1: {f1_per_class[i]:.4f}')

    results_dict = {'slide_id': slide_ids, 'Y': all_labels, 'Y_hat': all_preds}
    for c in range(self.n_classes):
      results_dict.update({'p_{}'.format(c): all_probs[:,c]})
    
    if self.verbose:
      self.logger.debug("Results dictionary created")
    
    df = pd.DataFrame(results_dict)

    return patient_results, test_error, auc, acc_logger, df, f1, metrics

  def eval_model(self, ckpt_path):
    self.logger.info(f"Loading model checkpoint from: {ckpt_path}", timestamp=True)
    
    if hasattr(self.model, 'load_model'):
      if self.verbose: 
        self.logger.info('Using built-in API to load the checkpoint...')
      self.model.load_model(ckpt_path)
    else:
      ckpt = torch.load(ckpt_path, map_location = 'cpu')
      msg = self.model.load_state_dict(ckpt)
      self.logger.info('Loading results: {}'.format(msg))
        
    self.logger.info("Running evaluation on test data...", timestamp=True)
    func = self.summary_subtyping
    patient_results, test_error, auc, acc_logger, df, f1, metrics = func(self.test_loader)
    
    self.logger.success(f"Evaluation complete - Error: {test_error:.4f}, AUC: {auc:.4f}, F1: {f1:.4f}", timestamp=True)
    return patient_results, test_error, auc, acc_logger, df, f1, metrics

[FILE-PATH: utils/__init__.py]
[CODE]


[FILE-PATH: utils/clam_utils/__init__.py]
[CODE]
import collections
import numpy as np
import torch
from itertools import islice
from torch.utils.data import DataLoader, Sampler, WeightedRandomSampler, RandomSampler, SequentialSampler

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SubsetSequentialSampler(Sampler):
	def __init__(self, indices):
		self.indices = indices

	def __iter__(self):
		return iter(self.indices)

	def __len__(self):
		return len(self.indices)

def collate_MIL(batch):
  # for other MIL methods
  if len(batch) == 1: return batch[0]
  else: raise RuntimeError

def generate_split(
  cls_ids,
  val_num, 
  test_num, 
  samples,
  n_splits = 5,
	seed = 7,
):
  indices = np.arange(samples).astype(int)
  np.random.seed(seed)

  for i in range(n_splits):
    all_val_ids = []
    all_test_ids = []
    sampled_train_ids = []
		
    for c in range(len(val_num)):
      possible_indices = np.intersect1d(cls_ids[c], indices) # all indices of this class
      val_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids

      remaining_ids = np.setdiff1d(possible_indices, val_ids) # indices of this class left after validation
      all_val_ids.extend(val_ids)

      test_ids = np.random.choice(remaining_ids, test_num[c], replace = False)
      remaining_ids = np.setdiff1d(remaining_ids, test_ids)
      all_test_ids.extend(test_ids)

      sampled_train_ids.extend(remaining_ids)

    yield sampled_train_ids, all_val_ids, all_test_ids

def nth(iterator, n, default=None):
	if n is None:
		return collections.deque(iterator, maxlen=0)
	else:
		return next(islice(iterator,n, None), default)

def print_network(net):
	num_params = 0
	num_params_train = 0
	print(net)
	
	for param in net.parameters():
		n = param.numel()
		num_params += n
		if param.requires_grad:
			num_params_train += n
	
	print('Total number of parameters: %d' % num_params)
	print('Total number of trainable parameters: %d' % num_params_train)

def make_weights_for_balanced_classes_split(dataset):
	N = float(len(dataset))
	weight_per_class = [
    N / len(dataset.slide_cls_ids[c]) if len(dataset.slide_cls_ids[c]) > 0 else 1e-8
    for c in range(len(dataset.slide_cls_ids))
  ]

	weight = [0] * int(N)
	for idx in range(len(dataset)):
		y = dataset.getlabel(idx)
		weight[idx] = weight_per_class[y]

	return torch.DoubleTensor(weight)

def get_split_loader(split_dataset, training = False, testing = False, weighted = False, batch_size=1):
	"""
		return either the validation loader or training loader 
	"""
	kwargs = {'num_workers': 0} if device.type == "cuda" else {}
	if not testing:
		if training:
			if weighted:
				weights = make_weights_for_balanced_classes_split(split_dataset)
				loader = DataLoader(split_dataset, batch_size=batch_size, sampler = WeightedRandomSampler(weights, len(weights)), collate_fn = collate_MIL, **kwargs)	
			else:
				loader = DataLoader(split_dataset, batch_size=batch_size, sampler = RandomSampler(split_dataset), collate_fn = collate_MIL, **kwargs)
		else:
			loader = DataLoader(split_dataset, batch_size=batch_size, sampler = SequentialSampler(split_dataset), collate_fn = collate_MIL, **kwargs)
	
	else:
		ids = np.random.choice(np.arange(len(split_dataset), int(len(split_dataset)*0.1)), replace = False)
		loader = DataLoader(split_dataset, batch_size=1, sampler = SubsetSequentialSampler(ids), collate_fn = collate_MIL, **kwargs )

	return loader

def calculate_error(Y_hat, Y):
	error = 1. - Y_hat.float().eq(Y.float()).float().mean().item()
	return error

[FILE-PATH: utils/wsi_core/batch_process_utils.py]
[CODE]
import pandas as pd
import numpy as np

def initialize_dataframe(
  slides: list,
  seg_params: dict,
  filter_params: dict,
  vis_params: dict,
  patch_params: dict,
	use_heatmap_args: bool = False,
  save_patches: bool = False
):
	total_slides = len(slides)
	default_df_dict = {
    'slide_id': slides,
    'process': np.full((total_slides), 1, dtype = np.uint8)
  }

	# initiate empty labels in case not provided
	if use_heatmap_args:
		default_df_dict.update({'label': np.full((total_slides), -1)})
	
	default_df_dict.update({
		'status': np.full((total_slides), 'tbp'),

    # segmentation params
		'seg_level': np.full((total_slides), int(seg_params['seg_level']), dtype = np.int8),
		'sthresh': np.full((total_slides), int(seg_params['sthresh']), dtype = np.uint8),
		'mthresh': np.full((total_slides), int(seg_params['mthresh']), dtype = np.uint8),
		'close': np.full((total_slides), int(seg_params['close']), dtype = np.uint32),
		'use_otsu': np.full((total_slides), bool(seg_params['use_otsu']), dtype = bool),
		'keep_ids': np.full((total_slides), seg_params['keep_ids']),
		'exclude_ids': np.full((total_slides), seg_params['exclude_ids']),
		
		# filter params
		'a_t': np.full((total_slides), int(filter_params['a_t']), dtype = np.float32),
		'a_h': np.full((total_slides), int(filter_params['a_h']), dtype = np.float32),
		'max_n_holes': np.full((total_slides), int(filter_params['max_n_holes']), dtype = np.uint32),

		# vis params
		'vis_level': np.full((total_slides), int(vis_params['vis_level']), dtype = np.int8),
		'line_thickness': np.full((total_slides), int(vis_params['line_thickness']), dtype = np.uint32),

		# patching params
		'use_padding': np.full((total_slides), bool(patch_params['use_padding']), dtype = bool),
		'contour_fn': np.full((total_slides), patch_params['contour_fn'])
  })

	if save_patches:
		default_df_dict.update({
			'white_thresh': np.full((total_slides), int(patch_params['white_thresh']), dtype = np.uint8),
			'black_thresh': np.full((total_slides), int(patch_params['black_thresh']), dtype = np.uint8)
    })

	if use_heatmap_args:
		# initiate empty x,y coordinates in case not provided
		default_df_dict.update({
      'x1': np.empty((total_slides)).fill(np.NaN), 
			'x2': np.empty((total_slides)).fill(np.NaN), 
			'y1': np.empty((total_slides)).fill(np.NaN), 
			'y2': np.empty((total_slides)).fill(np.NaN)
    })
	
	return pd.DataFrame(default_df_dict)

[FILE-PATH: utils/wsi_core/whole_slide_image.py]
[CODE]
import math
import os
import time
import cv2
import openslide
import pickle
import numpy as np
import xml.etree.ElementTree as ET
import multiprocessing as mp
import matplotlib.pyplot as plt
from xml.dom import minidom
from PIL import Image

Image.MAX_IMAGE_PIXELS = 200000000000

from utils.wsi_core.wsi_utils import save_patch_iter_bag_hdf5, \
  initialize_hdf5_bag, \
  save_hdf5, \
  screen_coords, \
  is_black_patch, \
  is_white_patch, \
  to_percentiles

from utils.wsi_core.wsi_util_classes import IsInContourV1, \
  IsInContourV2, \
  IsInContourV3Easy, \
  IsInContourV3Hard, \
  ContourCheckingFn

from utils.file_utils import load_pkl, save_pkl

class ImgReader:
  default_dims = [1.0, 2.0, 4.0, 8.0, 16.0]
  
  def __init__(
    self,
    filename,
    verbose = False
  ) -> None:
    self.verbose = verbose
    self.filename = filename
    dtype = filename.split('.')[-1]
    
    if dtype in ['tif', 'svs','ndpi', 'tiff']:
      self.openslide = True
      self.handle = openslide.OpenSlide(filename)
      self._shape = self.handle.level_dimensions[0]
    else:
      self.openslide = False
      img = cv2.imread(filename)[:, :, ::-1] # to RGB
      h, w, _ = img.shape
      # openslide (width, height)
      self.img = img
      self._shape = [w, h]

  def read_region(
    self,
    location,
    level,
    size
  ):
    # convert coors, the coors always on level 0
    x, y = location
    w, h = size
    _w = int(w*self.level_downsamples[level])
    _h = int(h*self.level_downsamples[level])

    if self.openslide:
      img = self.handle.read_region(location, 0, (_w, _h)).resize((w, h)).convert('RGB')
    else:
      img = self.img[y: y + _h, x: x + _w].copy()
      img = Image.fromarray(img).resize((w, h))
    return img
  
  def __read(
    self,
    location,
    level,
    size
  ):
    w, h = size
    _w = int(w*self.level_downsamples[level])
    _h = int(h*self.level_downsamples[level])
    r = 1/self.level_downsamples[level]

    if _w < 20000 or _h < 20000:
      img = self.handle.read_region(location, 0, (_w, _h)).resize((w, h))
    else:
      step = 10000
      img = []
      x, y = location
      ex, ey = _w + x, _h + y
      
      xx = list(range(x, ex, step))
      xx = xx if ex in xx else xx + [ex]
      yy = list(range(y, ey, step))
      yy = yy if ey in yy else yy + [ey]

      # top to down
      counter = 0
      for _yy in yy:
        temp = []
        for _xx in xx:
          t = np.array(self.handle.read_region((_xx, _yy), 0, (step, step)))
          t = cv2.resize(t, None, fx=r, fy=r)
          temp.append(t)
          counter += 1
          if self.verbose: print(counter, len(yy)*len(xx))
        temp = np.concatenate(temp, axis=1)
        img.append(temp)

      img = np.concatenate(img, axis=0)
      img = Image.fromarray(img)
    return img

  @property
  def dimensions(self):
    return self.level_dimensions[0]

  @property
  def level_count(self):
    return len(self.default_dims)
  
  @property
  def level_downsamples(self):
    shape = [self._shape[0]/r[0] for r in self.level_dimensions]
    return shape
  
  @property
  def level_dimensions(self):
    shape = [(int(self._shape[0]/r), int(self._shape[1]/r)) for r in self.default_dims]
    return shape
  
  def get_best_level_for_downsample(self, scale):
    preset = [i*i for i in self.level_downsamples]
    err = [abs(i-scale) for i in preset]
    level = err.index(min(err))
    return level

  def close(self):
    pass

class WholeSlideImage(object):
    def __init__(
      self,
      path,
      verbose = False
    ):
      self.verbose = verbose
      self.name = os.path.splitext(os.path.basename(path))[0]
      self.wsi = openslide.open_slide(path)

      level_dim = self.wsi.level_dimensions
      if self.verbose:
        print(self.wsi.level_dimensions)
        print(level_dim)
      
      if len(level_dim) == 1:
        if self.verbose: print('ImgReader is adopted to speed up ...')
        self.wsi = ImgReader(path, verbose = self.verbose)
  
      self.level_downsamples = self._assert_level_downsamples()
      self.level_dim = self.wsi.level_dimensions
  
      self.contours_tissue = None
      self.contours_tumor = None
      self.hdf5_file = None
      if self.verbose: print("WSI Reader is initialized ...")

    def get_open_slide(self):
      return self.wsi

    def init_XML(
      self,
      xml_path
    ):
      def _create_contour(coord_list):
        return np.array(
          [
            [
              [
                int(float(coord.attributes['X'].value)), 
                int(float(coord.attributes['Y'].value))
              ]
            ] for coord in coord_list
          ], 
          dtype = 'int32'
        )

      xmldoc = minidom.parse(xml_path)
      annotations = [anno.getElementsByTagName('Coordinate') for anno in xmldoc.getElementsByTagName('Annotation')]
      self.contours_tumor  = [_create_contour(coord_list) for coord_list in annotations]
      self.contours_tumor = sorted(self.contours_tumor, key=cv2.contourArea, reverse=True)

    def init_txt(
      self,
      annot_path
    ):
      def _create_contours_from_dict(annot):
        all_cnts = []
        
        for idx, annot_group in enumerate(annot):
          contour_group = annot_group['coordinates']
          if annot_group['type'] == 'Polygon':
            for idx, contour in enumerate(contour_group):
              contour = np.array(contour).astype(np.int32).reshape(-1,1,2)
              all_cnts.append(contour) 
          else:
            for idx, sgmt_group in enumerate(contour_group):
              contour = []
              for sgmt in sgmt_group: 
                contour.extend(sgmt)

              contour = np.array(contour).astype(np.int32).reshape(-1,1,2)    
              all_cnts.append(contour) 
              
        return all_cnts
      
      with open(annot_path, "r") as f:
        annot = f.read()
        annot = eval(annot)

      self.contours_tumor  = _create_contours_from_dict(annot)
      self.contours_tumor = sorted(self.contours_tumor, key=cv2.contourArea, reverse=True)

    def init_segmentation(
      self,
      mask_file
    ):
      # load segmentation results from pickle file
      asset_dict = load_pkl(mask_file)
      self.holes_tissue = asset_dict['holes']
      self.contours_tissue = asset_dict['tissue']

    def save_segmentation(
      self,
      mask_file
    ):
      # save segmentation results using pickle
      asset_dict = {
        'holes': self.holes_tissue,
        'tissue': self.contours_tissue
      }
      save_pkl(mask_file, asset_dict)

    def segment_tissue(
      self, 
      seg_level = 0,
      sthresh = 20,
      sthresh_up = 255,
      mthresh = 7,
      close = 0,
      use_otsu = False,
      filter_params = { 'a_t': 100 },
      ref_patch_size = 512,
      exclude_ids = [],
      keep_ids = []
    ):
      """
        Segment the tissue via HSV -> Median thresholding -> Binary threshold
      """
      
      def _filter_contours(
        contours,
        hierarchy,
        filter_params
      ):
        """
          Filter contours by: area.
        """
        filtered = []

        # find indices of foreground contours (parent == -1)
        hierarchy_1 = np.flatnonzero(hierarchy[:, 1] == -1)
        all_holes = []
        
        # loop through foreground contour indices
        for cont_idx in hierarchy_1:
          # actual contour
          cont = contours[cont_idx]
          
          # indices of holes contained in this contour (children of parent contour)
          holes = np.flatnonzero(hierarchy[:, 1] == cont_idx)
          
          # take contour area (includes holes)
          a = cv2.contourArea(cont)
          
          # calculate the contour area of each hole
          hole_areas = [cv2.contourArea(contours[hole_idx]) for hole_idx in holes]
          
          # actual area of foreground contour region
          a = a - np.array(hole_areas).sum()
          
          if a == 0: continue
          if tuple((filter_params['a_t'],)) < tuple((a,)): 
            filtered.append(cont_idx)
            all_holes.append(holes)

        foreground_contours = [contours[cont_idx] for cont_idx in filtered]
        hole_contours = []

        for hole_ids in all_holes:
          unfiltered_holes = [contours[idx] for idx in hole_ids ]
          unfilered_holes = sorted(
            unfiltered_holes,
            key = cv2.contourArea,
            reverse = True
          )
          
          # take max_n_holes largest holes by area
          unfilered_holes = unfilered_holes[:filter_params['max_n_holes']]
          filtered_holes = []
          
          # filter these holes
          for hole in unfilered_holes:
            if cv2.contourArea(hole) > filter_params['a_h']:
              filtered_holes.append(hole)

          hole_contours.append(filtered_holes)
        return foreground_contours, hole_contours
      
      img = np.array(self.wsi.read_region((0,0), seg_level, self.level_dim[seg_level]))
      img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)  # convert to HSV space
      img_med = cv2.medianBlur(img_hsv[:,:,1], mthresh)  # apply median blurring
      
      # thresholding
      if use_otsu:
        _, img_otsu = cv2.threshold(
          img_med,
          0,
          sthresh_up,
          cv2.THRESH_OTSU + cv2.THRESH_BINARY
        )
      else:
        _, img_otsu = cv2.threshold(
          img_med,
          sthresh,
          sthresh_up,
          cv2.THRESH_BINARY
        )

      # ,orphological closing
      if close > 0:
        kernel = np.ones((close, close), np.uint8)
        img_otsu = cv2.morphologyEx(img_otsu, cv2.MORPH_CLOSE, kernel)                 

      scale = self.level_downsamples[seg_level]
      scaled_ref_patch_area = int(ref_patch_size**2 / (scale[0] * scale[1]))
      filter_params = filter_params.copy()
      
      filter_params['a_t'] = filter_params['a_t'] * scaled_ref_patch_area
      filter_params['a_h'] = filter_params['a_h'] * scaled_ref_patch_area
      
      # to find and filter contours
      contours, hierarchy = cv2.findContours(img_otsu, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE) # find contours 
      hierarchy = np.squeeze(hierarchy, axis=(0,))[:, 2:]
      
      if filter_params: 
        # necessary for filtering out artifacts
        foreground_contours, hole_contours = _filter_contours(contours, hierarchy, filter_params)  

      self.contours_tissue = self.scale_contour_dim(foreground_contours, scale)
      self.holes_tissue = self.scale_holes_dim(hole_contours, scale)

      if len(keep_ids) > 0:
        contour_ids = set(keep_ids) - set(exclude_ids)
      else:
        contour_ids = set(np.arange(len(self.contours_tissue))) - set(exclude_ids)

      self.contours_tissue = [self.contours_tissue[i] for i in contour_ids]
      self.holes_tissue = [self.holes_tissue[i] for i in contour_ids]
      self.__vis_wsi_cache = img

    def vis_wsi(
      self,
      vis_level = 0,
      color = (0, 255, 0),
      hole_color = (0, 0, 255),
      annot_color = (255, 0, 0), 
      line_thickness = 250,
      max_size = None,
      top_left = None,
      bot_right = None,
      custom_downsample = 1,
      view_slide_only = False,
      number_contours = False,
      seg_display = True,
      annot_display = True
    ):
      downsample = self.level_downsamples[vis_level]
      scale = [1/downsample[0], 1/downsample[1]]
      
      if top_left is not None and bot_right is not None:
        top_left = tuple(top_left)
        bot_right = tuple(bot_right)
        
        w, h = tuple((np.array(bot_right) * scale).astype(int) - (np.array(top_left) * scale).astype(int))
        region_size = (w, h)
        
        # read specific region
        img = np.array(self.wsi.read_region(top_left, vis_level, region_size).convert("RGB"))
      else:
        top_left = (0,0)
        region_size = self.level_dim[vis_level]
        
        # may produce wrong results, make sure the seg_level is consistent with the vis_level
        img = self.__vis_wsi_cache

      if not view_slide_only:
        offset = tuple(-(np.array(top_left) * scale).astype(int))
        line_thickness = int(line_thickness * math.sqrt(scale[0] * scale[1]))
        
        if self.contours_tissue is not None and seg_display:
          if not number_contours:
            cv2.drawContours(
              img,
              self.scale_contour_dim(self.contours_tissue, scale), 
              -1,
              color,
              line_thickness,
              lineType = cv2.LINE_8,
              offset = offset
            )
          else: # just to add numbering to each contour
            for idx, cont in enumerate(self.contours_tissue):
              contour = np.array(self.scale_contour_dim(cont, scale))
              M = cv2.moments(contour)
              cX = int(M["m10"] / (M["m00"] + 1e-9))
              cY = int(M["m01"] / (M["m00"] + 1e-9))
              
              # draw the contour and put text next to center
              cv2.drawContours(
                img,
                [contour],
                -1,
                color,
                line_thickness,
                lineType = cv2.LINE_8,
                offset = offset
              )
              
              cv2.putText(
                img, 
                "{}".format(idx),
                (cX, cY),
                cv2.FONT_HERSHEY_SIMPLEX,
                2,
                (255, 0, 0),
                10
              )

          for holes in self.holes_tissue:
            cv2.drawContours(
              img,
              self.scale_contour_dim(holes, scale), 
              -1,
              hole_color,
              line_thickness,
              lineType = cv2.LINE_8
            )
        
        if self.contours_tumor is not None and annot_display:
          cv2.drawContours(
            img,
            self.scale_contour_dim(self.contours_tumor, scale), 
            -1,
            annot_color,
            line_thickness,
            lineType = cv2.LINE_8,
            offset = offset
          )
      
      img = Image.fromarray(img).convert('RGB')
  
      w, h = img.size
      if custom_downsample > 1:
        img = img.resize((int(w/custom_downsample), int(h/custom_downsample)))

      if max_size is not None and (w > max_size or h > max_size):
        resizeFactor = max_size/w if w > h else max_size/h
        img = img.resize((int(w*resizeFactor), int(h*resizeFactor)))
      
      return img

    def create_patches_bag_hdf5(
      self,
      save_path,
      patch_level = 0,
      patch_size = 256,
      step_size = 256,
      save_coord = True,
      **kwargs
    ):
      contours = self.contours_tissue
      contour_holes = self.holes_tissue

      if self.verbose: print("Creating patches for: ", self.name, "...",)
      elapsed = time.time()
      
      for idx, cont in enumerate(contours):
        patch_gen = self._get_patch_generator(cont, idx, patch_level, save_path, patch_size, step_size, **kwargs)
        
        if self.hdf5_file is None:
          try: first_patch = next(patch_gen)
          except StopIteration: continue

          file_path = initialize_hdf5_bag(first_patch, save_coord=save_coord)
          self.hdf5_file = file_path

        for patch in patch_gen:
          save_patch_iter_bag_hdf5(patch)

      return self.hdf5_file

    def _get_patch_generator(
      self,
      cont,
      cont_idx,
      patch_level,
      save_path,
      patch_size = 256,
      step_size = 256,
      custom_downsample = 1,
      white_black = True,
      white_thresh = 15,
      black_thresh = 50,
      contour_fn = 'four_pt',
      use_padding = True
    ):
      start_x, start_y, w, h = cv2.boundingRect(cont) if cont is not None else (0, 0, self.level_dim[patch_level][0], self.level_dim[patch_level][1])
      if self.verbose: print("Bounding Box:", start_x, start_y, w, h)
      if self.verbose: print("Contour Area:", cv2.contourArea(cont))
      
      if custom_downsample > 1:
        assert custom_downsample == 2 
        target_patch_size = patch_size
        patch_size = target_patch_size * 2
        step_size = step_size * 2
        if self.verbose: print("Custom Downsample: {}, Patching at {} x {}, But Final Patch Size is {} x {}".format(
          custom_downsample, patch_size, patch_size, target_patch_size, target_patch_size))

      patch_downsample = (int(self.level_downsamples[patch_level][0]), int(self.level_downsamples[patch_level][1]))
      ref_patch_size = (patch_size*patch_downsample[0], patch_size*patch_downsample[1])
      
      step_size_x = step_size * patch_downsample[0]
      step_size_y = step_size * patch_downsample[1]
      
      if isinstance(contour_fn, str):
        if contour_fn == 'four_pt':
          cont_check_fn = IsInContourV3Easy(contour=cont, patch_size=ref_patch_size[0], center_shift=0.5)
        elif contour_fn == 'four_pt_hard':
          cont_check_fn = IsInContourV3Hard(contour=cont, patch_size=ref_patch_size[0], center_shift=0.5)
        elif contour_fn == 'center':
          cont_check_fn = IsInContourV2(contour=cont, patch_size=ref_patch_size[0])
        elif contour_fn == 'basic':
          cont_check_fn = IsInContourV1(contour=cont)
        else:
            raise NotImplementedError
      else:
        assert isinstance(contour_fn, ContourCheckingFn)
        cont_check_fn = contour_fn

      img_w, img_h = self.level_dim[0]
      if use_padding:
        stop_y = start_y+h
        stop_x = start_x+w
      else:
        stop_y = min(start_y+h, img_h-ref_patch_size[1])
        stop_x = min(start_x+w, img_w-ref_patch_size[0])

      count = 0
      for y in range(start_y, stop_y, step_size_y):
        for x in range(start_x, stop_x, step_size_x):
          if not self.is_in_contours(cont_check_fn, (x,y), self.holes_tissue[cont_idx], ref_patch_size[0]): 
            # point not inside contour and its associated holes
            continue
          
          count += 1
          patch_PIL = self.wsi.read_region((x,y), patch_level, (patch_size, patch_size)).convert('RGB')
          if custom_downsample > 1:
            patch_PIL = patch_PIL.resize((target_patch_size, target_patch_size))
          
          if white_black:
            if is_black_patch(np.array(patch_PIL), rgbThresh = black_thresh) or \
              is_white_patch(np.array(patch_PIL), satThresh = white_thresh): 
                continue

          patch_info = {
            'x': x // (patch_downsample[0] * custom_downsample), 
            'y': y // (patch_downsample[1] * custom_downsample),
            'cont_idx': cont_idx,
            'patch_level': patch_level, 
            'downsample': self.level_downsamples[patch_level],
            'downsampled_level_dim': tuple(np.array(self.level_dim[patch_level])//custom_downsample),
            'level_dim': self.level_dim[patch_level],
            'patch_PIL': patch_PIL,
            'name': self.name,
            'save_path': save_path
          }

          yield patch_info
      if self.verbose: print("patches extracted: {}".format(count))

    @staticmethod
    def is_in_holes(
      holes,
      pt,
      patch_size
    ):
      for hole in holes:
        if cv2.pointPolygonTest(hole, (pt[0]+patch_size/2, pt[1]+patch_size/2), False) > 0:
          return 1
      return 0

    @staticmethod
    def is_in_contours(
      cont_check_fn,
      pt,
      holes = None,
      patch_size = 256
    ):
      if cont_check_fn(pt):
        if holes is not None: return not WholeSlideImage.is_in_holes(holes, pt, patch_size)
        else: return 1
      return 0
    
    @staticmethod
    def scale_contour_dim(
      contours,
      scale
    ):
      return [np.array(cont * scale, dtype='int32') for cont in contours]

    @staticmethod
    def scale_holes_dim(
      contours,
      scale
    ):
      return [[np.array(hole * scale, dtype = 'int32') for hole in holes] for holes in contours]

    def _assert_level_downsamples(self):
      level_downsamples = []
      dim_0 = self.wsi.level_dimensions[0]
      
      for downsample, dim in zip(self.wsi.level_downsamples, self.wsi.level_dimensions):
        estimated_downsample = (dim_0[0]/float(dim[0]), dim_0[1]/float(dim[1]))
        level_downsamples.append(estimated_downsample) if estimated_downsample != (downsample, downsample) else level_downsamples.append((downsample, downsample))
      
      return level_downsamples

    def process_contours(
      self,
      save_path,
      patch_level = 0,
      patch_size = 256,
      step_size = 256,
      **kwargs
    ):
      save_path_hdf5 = os.path.join(save_path, str(self.name) + '.h5')
      
      if self.verbose: print("Creating patches for: ", self.name, "...",)
      elapsed = time.time()
      n_contours = len(self.contours_tissue)
      
      if self.verbose: print("Total number of contours to process: ", n_contours)
      fp_chunk_size = math.ceil(n_contours * 0.05)
      init = True
      
      for idx, cont in enumerate(self.contours_tissue):
        if (idx + 1) % fp_chunk_size == fp_chunk_size:
          if self.verbose: print('Processing contour {}/{}'.format(idx, n_contours))
        
        asset_dict, attr_dict = self.process_contour(
          cont,
          self.holes_tissue[idx],
          patch_level,
          save_path,
          patch_size,
          step_size,
          **kwargs
        )
        
        if len(asset_dict) > 0:
          if init:
            save_hdf5(save_path_hdf5, asset_dict, attr_dict, mode='w')
            init = False
          else:
            save_hdf5(save_path_hdf5, asset_dict, mode='a')

      return self.hdf5_file

    def process_contour(
      self,
      cont,
      contour_holes,
      patch_level,
      save_path,
      patch_size = 256,
      step_size = 256,
      contour_fn = 'four_pt',
      use_padding = True,
      top_left = None,
      bot_right = None
    ):
      start_x, start_y, w, h = cv2.boundingRect(cont) if cont is not None else (0, 0, self.level_dim[patch_level][0], self.level_dim[patch_level][1])

      patch_downsample = (int(self.level_downsamples[patch_level][0]), int(self.level_downsamples[patch_level][1]))
      ref_patch_size = (patch_size*patch_downsample[0], patch_size*patch_downsample[1])
      
      img_w, img_h = self.level_dim[0]
      if use_padding:
        stop_y = start_y + h
        stop_x = start_x + w
      else:
        stop_y = min(start_y + h, img_h - ref_patch_size[1] + 1)
        stop_x = min(start_x + w, img_w - ref_patch_size[0] + 1)
      
      if self.verbose: print("Bounding Box:", start_x, start_y, w, h)
      if self.verbose: print("Contour Area:", cv2.contourArea(cont))

      if bot_right is not None:
        stop_y = min(bot_right[1], stop_y)
        stop_x = min(bot_right[0], stop_x)
      if top_left is not None:
        start_y = max(top_left[1], start_y)
        start_x = max(top_left[0], start_x)

      if bot_right is not None or top_left is not None:
        w, h = stop_x - start_x, stop_y - start_y
        if w <= 0 or h <= 0:
          if self.verbose: print("Contour is not in specified ROI, skip")
          return {}, {}
        else:
          if self.verbose: print("Adjusted Bounding Box:", start_x, start_y, w, h)
  
      if isinstance(contour_fn, str):
        if contour_fn == 'four_pt':
          cont_check_fn = IsInContourV3Easy(contour=cont, patch_size=ref_patch_size[0], center_shift=0.5)
        elif contour_fn == 'four_pt_hard':
          cont_check_fn = IsInContourV3Hard(contour=cont, patch_size=ref_patch_size[0], center_shift=0.5)
        elif contour_fn == 'center':
          cont_check_fn = IsInContourV2(contour=cont, patch_size=ref_patch_size[0])
        elif contour_fn == 'basic':
          cont_check_fn = IsInContourV1(contour=cont)
        else:
          raise NotImplementedError
      else:
        assert isinstance(contour_fn, ContourCheckingFn)
        cont_check_fn = contour_fn

      step_size_x = step_size * patch_downsample[0]
      step_size_y = step_size * patch_downsample[1]

      x_range = np.arange(start_x, stop_x, step=step_size_x)
      y_range = np.arange(start_y, stop_y, step=step_size_y)
      x_coords, y_coords = np.meshgrid(x_range, y_range, indexing='ij')
      coord_candidates = np.array([x_coords.flatten(), y_coords.flatten()]).transpose()

      num_workers = mp.cpu_count()
      if num_workers > 4: num_workers = 4
      pool = mp.Pool(num_workers)

      iterable = [(coord, contour_holes, ref_patch_size[0], cont_check_fn) for coord in coord_candidates]
      results = pool.starmap(WholeSlideImage.process_coord_candidate, iterable)
      pool.close()
      results = np.array([result for result in results if result is not None])
      
      if self.verbose: print('Extracted {} coordinates'.format(len(results)))

      if len(results) > 1:
        asset_dict = { 'coords': results }
        attr = {
          'patch_size': patch_size,
          'patch_level': patch_level,
          'downsample': self.level_downsamples[patch_level],
          'downsampled_level_dim': tuple(np.array(self.level_dim[patch_level])),
          'level_dim': self.level_dim[patch_level],
          'name': self.name,
          'save_path': save_path
        }

        attr_dict = { 'coords': attr }
        return asset_dict, attr_dict
      else:
        return {}, {}

    @staticmethod
    def process_coord_candidate(
      coord,
      contour_holes,
      ref_patch_size,
      cont_check_fn
    ):
      if WholeSlideImage.is_in_contours(cont_check_fn, coord, contour_holes, ref_patch_size):
        return coord
      else:
        return None

    def vis_heatmap(
      self,
      scores,
      coords,
      vis_level = -1, 
      top_left = None,
      bot_right = None,
      patch_size = (256, 256), 
      blank_canvas = False,
      canvas_color = (220, 20, 50),
      alpha = 0.4,
      blur = False,
      overlap = 0.0, 
      segment = True,
      use_holes = True,
      convert_to_percentiles = False,
      binarize = False,
      thresh = 0.5,
      max_size = None,
      custom_downsample = 1,
      cmap = 'coolwarm'
    ):
      """
      Args:
        scores (numpy array of float): Attention scores 
        coords (numpy array of int, n_patches x 2): Corresponding coordinates (relative to lvl 0)
        vis_level (int): WSI pyramid level to visualize
        patch_size (tuple of int): Patch dimensions (relative to lvl 0)
        blank_canvas (bool): Whether to use a blank canvas to draw the heatmap (vs. using the original slide)
        canvas_color (tuple of uint8): Canvas color
        alpha (float [0, 1]): blending coefficient for overlaying heatmap onto original slide
        blur (bool): apply gaussian blurring
        overlap (float [0 1]): percentage of overlap between neighboring patches (only affect radius of blurring)
        segment (bool): whether to use tissue segmentation contour (must have already called self.segmentTissue such that 
                        self.contours_tissue and self.holes_tissue are not None
        use_holes (bool): whether to also clip out detected tissue cavities (only in effect when segment == True)
        convert_to_percentiles (bool): whether to convert attention scores to percentiles
        binarize (bool): only display patches > threshold
        threshold (float): binarization threshold
        max_size (int): Maximum canvas size (clip if goes over)
        custom_downsample (int): additionally downscale the heatmap by specified factor
        cmap (str): name of matplotlib colormap to use
      """

      if vis_level < 0:
        vis_level = self.wsi.get_best_level_for_downsample(32)

      downsample = self.level_downsamples[vis_level]
      scale = [1/downsample[0], 1/downsample[1]] # scaling from 0 to desired level

      if len(scores.shape) == 2:
        scores = scores.flatten()

      if binarize:
        if thresh < 0: threshold = 1.0 / len(scores)
        else: threshold =  thresh
      else:
        threshold = 0.0

      # calculate size of heatmap and filter coordinates/scores outside specified bbox region
      if top_left is not None and bot_right is not None:
        scores, coords = screen_coords(scores, coords, top_left, bot_right)
        coords = coords - top_left
        top_left = tuple(top_left)
        bot_right = tuple(bot_right)
        w, h = tuple((np.array(bot_right) * scale).astype(int) - (np.array(top_left) * scale).astype(int))
        region_size = (w, h)
      else:
        region_size = self.level_dim[vis_level]
        top_left = (0,0)
        bot_right = self.level_dim[0]
        w, h = region_size

      patch_size  = np.ceil(np.array(patch_size) * np.array(scale)).astype(int)
      coords = np.ceil(coords * np.array(scale)).astype(int)
      
      if self.verbose: print('\ncreating heatmap for: ')
      if self.verbose: print('top_left: ', top_left, 'bot_right: ', bot_right)
      if self.verbose: print('w: {}, h: {}'.format(w, h))
      if self.verbose: print('scaled patch size: ', patch_size)

      # normalize filtered scores 
      if convert_to_percentiles:
        scores = to_percentiles(scores) 
      scores /= 100
      
      # calculate the heatmap of raw attention scores (before colormap) 
      # by accumulating scores over overlapped regions
      # heatmap overlay: tracks attention score over each pixel of heatmap
      # overlay counter: tracks how many times attention score is accumulated over each pixel of heatmap
      overlay = np.full(np.flip(region_size), 0).astype(float)
      counter = np.full(np.flip(region_size), 0).astype(np.uint16)      
      
      count = 0
      for idx in range(len(coords)):
        score = scores[idx]
        coord = coords[idx]
        if score >= threshold:
          if binarize:
            score = 1.0
            count += 1
        else:
          score = 0.0
        
        # accumulate attention
        overlay[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] += score
        # accumulate counter
        counter[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] += 1

      if binarize:
        if self.verbose: print('\nbinarized tiles based on cutoff of {}'.format(threshold))
        if self.verbose: print('identified {}/{} patches as positive'.format(count, len(coords)))
      
      # fetch attended region and average accumulated attention
      zero_mask = counter == 0

      if binarize:
        overlay[~zero_mask] = np.around(overlay[~zero_mask] / counter[~zero_mask])
      else:
        overlay[~zero_mask] = overlay[~zero_mask] / counter[~zero_mask]

      del counter
      
      if blur:
        overlay = cv2.GaussianBlur(overlay,tuple((patch_size * (1-overlap)).astype(int) * 2 +1), 0)  

      if segment:
        tissue_mask = self.get_seg_mask(region_size, scale, use_holes=use_holes, offset=tuple(top_left))
      
      if not blank_canvas: # downsample original image and use as canvas
        img = np.array(self.wsi.read_region(top_left, vis_level, region_size).convert("RGB"))
      else: # use blank canvas 
        img = np.array(Image.new(size=region_size, mode="RGB", color=(255,255,255))) 

      if self.verbose: print('\ncomputing heatmap image')
      if self.verbose: print('total of {} patches'.format(len(coords)))
      twenty_percent_chunk = max(1, int(len(coords) * 0.2))

      if isinstance(cmap, str):
        cmap = plt.get_cmap(cmap)
      
      for idx in range(len(coords)):
        if (idx + 1) % twenty_percent_chunk == 0:
          if self.verbose: print('progress: {}/{}'.format(idx, len(coords)))
        
        score = scores[idx]
        coord = coords[idx]
        if score >= threshold:
          # attention block
          raw_block = overlay[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]]
          
          # image block (either blank canvas or orig image)
          img_block = img[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]].copy()

          # color block (cmap applied to attention block)
          color_block = (cmap(raw_block) * 255)[:,:,:3].astype(np.uint8)

          if segment:
            # tissue mask block
            mask_block = tissue_mask[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] 
            
            # copy over only tissue masked portion of color block
            img_block[mask_block] = color_block[mask_block]
          else:
            # copy over entire color block
            img_block = color_block

          # rewrite image block
          img[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0]] = img_block.copy()
      
      if self.verbose: print('Done')
      del overlay

      if blur:
        img = cv2.GaussianBlur(img,tuple((patch_size * (1-overlap)).astype(int) * 2 +1),0)  

      if alpha < 1.0:
        img = self.block_blending(img, vis_level, top_left, bot_right, alpha=alpha, blank_canvas=blank_canvas, block_size=1024)
      
      img = Image.fromarray(img)
      w, h = img.size

      if custom_downsample > 1:
        img = img.resize((int(w/custom_downsample), int(h/custom_downsample)))

      if max_size is not None and (w > max_size or h > max_size):
        resizeFactor = max_size/w if w > h else max_size/h
        img = img.resize((int(w*resizeFactor), int(h*resizeFactor)))
      
      return img
    
    def block_blending(
      self,
      img,
      vis_level,
      top_left,
      bot_right,
      alpha = 0.5,
      blank_canvas = False,
      block_size = 1024
    ):
      if self.verbose: print('\ncomputing blend')
      
      downsample = self.level_downsamples[vis_level]
      w = img.shape[1]
      h = img.shape[0]
      
      block_size_x = min(block_size, w)
      block_size_y = min(block_size, h)
      if self.verbose: print('using block size: {} x {}'.format(block_size_x, block_size_y))

      shift = top_left # amount shifted w.r.t. (0, 0)
      for x_start in range(top_left[0], bot_right[0], block_size_x * int(downsample[0])):
        for y_start in range(top_left[1], bot_right[1], block_size_y * int(downsample[1])):
          # 1. convert wsi coordinates to image coordinates via shift and scale
          x_start_img = int((x_start - shift[0]) / int(downsample[0]))
          y_start_img = int((y_start - shift[1]) / int(downsample[1]))
          
          # 2. compute end points of blend tile, careful not to go over the edge of the image
          y_end_img = min(h, y_start_img+block_size_y)
          x_end_img = min(w, x_start_img+block_size_x)

          if y_end_img == y_start_img or x_end_img == x_start_img: 
            continue
          
          # 3. fetch blend block and size
          blend_block = img[y_start_img:y_end_img, x_start_img:x_end_img] 
          blend_block_size = (x_end_img-x_start_img, y_end_img-y_start_img)
          
          if not blank_canvas: # 4. read actual wsi block as canvas block
            pt = (x_start, y_start)
            canvas = np.array(self.wsi.read_region(pt, vis_level, blend_block_size).convert("RGB"))     
          else: # 4. OR create blank canvas block
            canvas = np.array(Image.new(size=blend_block_size, mode="RGB", color=(255,255,255)))

          # 5. blend color block and canvas block
          img[y_start_img:y_end_img, x_start_img:x_end_img] = cv2.addWeighted(blend_block, alpha, canvas, 1 - alpha, 0, canvas)
      return img

    def get_seg_mask(
      self,
      region_size,
      scale,
      use_holes = False,
      offset = (0, 0)
    ):
      if self.verbose: print('\ncomputing foreground tissue mask')
      tissue_mask = np.full(np.flip(region_size), 0).astype(np.uint8)
      contours_tissue = self.scale_contour_dim(self.contours_tissue, scale)
      offset = tuple((np.array(offset) * np.array(scale) * -1).astype(np.int32))

      contours_holes = self.scale_holes_dim(self.holes_tissue, scale)
      contours_tissue, contours_holes = zip(
        *sorted(
          zip(contours_tissue, contours_holes), 
          key = lambda x: cv2.contourArea(x[0]),
          reverse = True
        )
      )
      
      for idx in range(len(contours_tissue)):
        cv2.drawContours(
          image = tissue_mask,
          contours = contours_tissue,
          contourIdx = idx,
          color = (1),
          offset = offset,
          thickness = -1
        )
        
        if use_holes:
          cv2.drawContours(
            image = tissue_mask,
            contours = contours_holes[idx],
            contourIdx = -1,
            color = (0),
            offset = offset,
            thickness = -1
          )
              
      tissue_mask = tissue_mask.astype(bool)
      if self.verbose: print('detected {}/{} of region as tissue'.format(tissue_mask.sum(), tissue_mask.size))
      
      return tissue_mask

[FILE-PATH: utils/wsi_core/wsi_utils.py]
[CODE]
import os
import numpy as np
import math
import cv2
import h5py
from scipy.stats import rankdata
from PIL import Image

Image.MAX_IMAGE_PIXELS = 200000000000

from utils.wsi_core.wsi_util_classes import MosaicCanvas

def is_white_patch(
  patch,
  satThresh = 5
):
  patch_hsv = cv2.cvtColor(patch, cv2.COLOR_RGB2HSV)
  return True if np.mean(patch_hsv[:,:,1]) < satThresh else False

def is_black_patch(
  patch,
  rgbThresh = 40
):
  return True if np.all(np.mean(patch, axis = (0,1)) < rgbThresh) else False

def is_white_patch_by_percentage(
  patch,
  rgbThresh = 20,
  percentage = 0.05
):
  num_pixels = patch.size[0] * patch.size[1]
  return True if np.all(np.array(patch) < rgbThresh, axis=(2)).sum() > num_pixels * percentage else False

def is_white_patch_by_percentage(
  patch,
  rgbThresh = 220,
  percentage = 0.2
):
  num_pixels = patch.size[0] * patch.size[1]
  return True if np.all(np.array(patch) > rgbThresh, axis=(2)).sum() > num_pixels * percentage else False

def coord_generator(
  x_start,
  x_end,
  x_step,
  y_start,
  y_end,
  y_step,
  args_dict = None
):
  for x in range(x_start, x_end, x_step):
    for y in range(y_start, y_end, y_step):
      if args_dict is not None:
        process_dict = args_dict.copy()
        process_dict.update({'pt':(x, y)})
        yield process_dict
      else:
        yield (x, y)

def save_patch_iter_bag_hdf5(patch):
  x, y, cont_idx, patch_level, downsample, downsampled_level_dim, level_dim, img_patch, name, save_path= tuple(patch.values())
  img_patch = np.array(img_patch)[np.newaxis, ...]
  img_shape = img_patch.shape

  file_path = os.path.join(save_path, name) + '.h5'
  file = h5py.File(file_path, 'a')

  dset = file['imgs']
  dset.resize(len(dset) + img_shape[0], axis=0)
  dset[-img_shape[0]:] = img_patch

  if 'coords' in file:
    coord_dset = file['coords']
    coord_dset.resize(len(coord_dset) + img_shape[0], axis=0)
    coord_dset[-img_shape[0]:] = (x,y)

  file.close()

def save_hdf5(
  output_path,
  asset_dict,
  attr_dict= None,
  mode='a'
):
  file = h5py.File(output_path, mode)
  for key, val in asset_dict.items():
    data_shape = val.shape
    
    if key not in file:
      data_type = val.dtype
      chunk_shape = (1, ) + data_shape[1:]
      maxshape = (None, ) + data_shape[1:]
      
      dset = file.create_dataset(
        key,
        shape = data_shape,
        maxshape = maxshape,
        chunks = chunk_shape,
        dtype = data_type
      )
      dset[:] = val

      if attr_dict is not None:
        if key in attr_dict.keys():
          for attr_key, attr_val in attr_dict[key].items():
            dset.attrs[attr_key] = attr_val
    else:
      dset = file[key]
      dset.resize(len(dset) + data_shape[0], axis=0)
      dset[-data_shape[0]:] = val

  file.close()
  return output_path

def initialize_hdf5_bag(
  first_patch,
  save_coord = False
):
  x, y, cont_idx, patch_level, downsample, downsampled_level_dim, level_dim, img_patch, name, save_path = tuple(first_patch.values())

  file_path = os.path.join(save_path, name) + '.h5'
  file = h5py.File(file_path, "w")

  img_patch = np.array(img_patch)[np.newaxis, ...]
  dtype = img_patch.dtype

  # to initialize a resizable dataset to hold the output
  img_shape = img_patch.shape
  maxshape = (None, ) + img_shape[1:] # maximum dimensions up to which dataset maybe resized (None means unlimited)
  dset = file.create_dataset(
    'imgs', 
    shape = img_shape,
    maxshape = maxshape,
    chunks = img_shape,
    dtype = dtype
  )

  dset[:] = img_patch
  dset.attrs['patch_level'] = patch_level
  dset.attrs['wsi_name'] = name
  dset.attrs['downsample'] = downsample
  dset.attrs['level_dim'] = level_dim
  dset.attrs['downsampled_level_dim'] = downsampled_level_dim

  if save_coord:
    coord_dset = file.create_dataset('coords', shape=(1, 2), maxshape=(None, 2), chunks=(1, 2), dtype=np.int32)
    coord_dset[:] = (x,y)

  file.close()
  return file_path

def sample_indices(
  scores,
  k,
  start = 0.48,
  end = 0.52,
  convert_to_percentile = False,
  seed = 1
):
  np.random.seed(seed)
  
  if convert_to_percentile:
    end_value = np.quantile(scores, end)
    start_value = np.quantile(scores, start)
  else:
    end_value = end
    start_value = start

  score_window = np.logical_and(scores >= start_value, scores <= end_value)
  indices = np.where(score_window)[0]
  
  if len(indices) < 1: return -1 
  else: return np.random.choice(indices, min(k, len(indices)), replace=False)

def top_k(scores, k, invert=False):
  if invert: top_k_ids = scores.argsort()[:k]
  else: top_k_ids = scores.argsort()[::-1][:k]
  return top_k_ids

def to_percentiles(scores):
  scores = rankdata(scores, 'average') / len(scores) * 100   
  return scores

def screen_coords(
  scores,
  coords,
  top_left,
  bot_right
):
  bot_right = np.array(bot_right)
  top_left = np.array(top_left)
  mask = np.logical_and(np.all(coords >= top_left, axis=1), np.all(coords <= bot_right, axis=1))
  scores = scores[mask]
  coords = coords[mask]
  return scores, coords

def sample_rois(
  scores,
  coords,
  k = 5,
  mode = 'range_sample',
  seed = 1,
  score_start = 0.45,
  score_end = 0.55,
  top_left = None,
  bot_right = None
):
  if len(scores.shape) == 2: 
    scores = scores.flatten()

  scores = to_percentiles(scores)
  if top_left is not None and bot_right is not None:
    scores, coords = screen_coords(scores, coords, top_left, bot_right)

  if mode == 'range_sample':
    sampled_ids = sample_indices(scores, start=score_start, end=score_end, k=k, convert_to_percentile=False, seed=seed)
  elif mode == 'topk':
    sampled_ids = top_k(scores, k, invert=False)
  elif mode == 'reverse_topk':
    sampled_ids = top_k(scores, k, invert=True)
  else:
      raise NotImplementedError

  coords = coords[sampled_ids]
  scores = scores[sampled_ids]

  asset = {
    'sampled_coords': coords,
    'sampled_scores': scores
  }
  
  return asset

def draw_grid(
  img,
  coord,
  shape,
  thickness = 2,
  color = (0, 0, 0, 255)
):
  cv2.rectangle(
    img,
    tuple(np.maximum([0, 0], coord-thickness//2)),
    tuple(coord - thickness//2 + np.array(shape)),
    (0, 0, 0, 255), 
    thickness = thickness
  )
  return img

def draw_map(
  canvas,
  patch_dset,
  coords,
  patch_size,
  indices = None,
  draw_grid = True,
  verbose = False
):
  if indices is None:
    indices = np.arange(len(coords))

  total = len(indices)
  if verbose:
    ten_percent_chunk = math.ceil(total * 0.1)
    print('start stitching {}'.format(patch_dset.attrs['wsi_name']))
  
  for idx in range(total):
    if verbose:
      if idx % ten_percent_chunk == 0:
        print('progress: {}/{} stitched'.format(idx, total))
    
    patch_id = indices[idx]
    patch = patch_dset[patch_id]
    
    patch = cv2.resize(patch, patch_size)
    coord = coords[patch_id]
    
    canvas_crop_shape = canvas[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0], :3].shape[:2]
    canvas[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0], :3] = patch[:canvas_crop_shape[0], :canvas_crop_shape[1], :]

    if draw_grid:
      draw_grid(canvas, coord, patch_size)

  return Image.fromarray(canvas)

def draw_map_from_coords(
  canvas,
  wsi_object,
  coords,
  patch_size,
  vis_level,
  indices = None,
  draw_grid = True,
  verbose = False
):
  downsamples = wsi_object.wsi.level_downsamples[vis_level]
  if indices is None:
    indices = np.arange(len(coords))

  total = len(indices)
  if verbose:
    ten_percent_chunk = math.ceil(total * 0.1)
      
  patch_size = tuple(np.ceil((np.array(patch_size)/np.array(downsamples))).astype(np.int32))
  if verbose: 
    print('downscaled patch size: {}x{}'.format(patch_size[0], patch_size[1]))
  
  for idx in range(total):
    if verbose:
      if idx % ten_percent_chunk == 0:
        print('progress: {}/{} stitched'.format(idx, total))
    
    patch_id = indices[idx]
    coord = coords[patch_id]
    
    patch = np.array(wsi_object.wsi.read_region(tuple(coord), vis_level, patch_size).convert("RGB"))
    coord = np.ceil(coord / downsamples).astype(np.int32)
    
    canvas_crop_shape = canvas[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0], :3].shape[:2]
    canvas[coord[1]:coord[1]+patch_size[1], coord[0]:coord[0]+patch_size[0], :3] = patch[:canvas_crop_shape[0], :canvas_crop_shape[1], :]

    if draw_grid:
      draw_grid(canvas, coord, patch_size)

  return Image.fromarray(canvas)

def stitch_patches(
  hdf5_file_path,
  downscale = 16,
  draw_grid = False,
  bg_color = (0, 0, 0),
  alpha = -1,
  verbose = False
):
  file = h5py.File(hdf5_file_path, 'r')
  dset = file['imgs']
  coords = file['coords'][:]

  if 'downsampled_level_dim' in dset.attrs.keys():
    w, h = dset.attrs['downsampled_level_dim']
  else:
    w, h = dset.attrs['level_dim']

  if verbose: 
    print('original size: {} x {}'.format(w, h))
  w = w // downscale
  h = h //downscale
  coords = (coords / downscale).astype(np.int32)

  if verbose: 
    print('downscaled size for stiching: {} x {}'.format(w, h))
    print('number of patches: {}'.format(len(dset)))
  img_shape = dset[0].shape

  if verbose: 
    print('patch shape: {}'.format(img_shape))
  downscaled_shape = (img_shape[1] // downscale, img_shape[0] // downscale)

  if w*h > Image.MAX_IMAGE_PIXELS: 
    raise Image.DecompressionBombError("Visualization Downscale %d is too large" % downscale)
  
  if alpha < 0 or alpha == -1:
    heatmap = Image.new(size=(w,h), mode="RGB", color=bg_color)
  else:
    heatmap = Image.new(size=(w,h), mode="RGBA", color=bg_color + (int(255 * alpha),))
  
  heatmap = np.array(heatmap)
  heatmap = draw_map(heatmap, dset, coords, downscaled_shape, indices=None, draw_grid=draw_grid)
  
  file.close()
  return heatmap

def stitch_coords(
  hdf5_file_path,
  wsi_object,
  downscale = 16,
  draw_grid = False,
  bg_color = (0, 0, 0),
  alpha = -1,
  verbose = False
):
  wsi = wsi_object.get_open_slide()
  vis_level = wsi.get_best_level_for_downsample(downscale)
  
  file = h5py.File(hdf5_file_path, 'r')
  dset = file['coords']
  coords = dset[:]
  w, h = wsi.level_dimensions[0]

  if verbose:
    print('start stitching {}'.format(dset.attrs['name']))
    print('original size: {} x {}'.format(w, h))

  w, h = wsi.level_dimensions[vis_level]
  if verbose:
    print('downscaled size for stiching: {} x {}'.format(w, h))
    print('number of patches: {}'.format(len(coords)))
  
  patch_size = dset.attrs['patch_size']
  patch_level = dset.attrs['patch_level']
  if verbose:
    print('patch size: {}x{} patch level: {}'.format(patch_size, patch_size, patch_level))
  
  patch_size = tuple((np.array((patch_size, patch_size)) * wsi.level_downsamples[patch_level]).astype(np.int32))
  if verbose:
    print('ref patch size: {}x{}'.format(patch_size, patch_size))
  
  if w*h > Image.MAX_IMAGE_PIXELS: 
    raise Image.DecompressionBombError("Visualization Downscale %d is too large" % downscale)
  
  if alpha < 0 or alpha == -1:
    heatmap = Image.new(size=(w,h), mode="RGB", color=bg_color)
  else:
    heatmap = Image.new(size=(w,h), mode="RGBA", color=bg_color + (int(255 * alpha),))
  
  heatmap = np.array(heatmap)
  heatmap = draw_map_from_coords(heatmap, wsi_object, coords, patch_size, vis_level, indices=None, draw_grid=draw_grid)
  
  file.close()
  return heatmap

def sample_patches(
  coords_file_path,
  save_file_path,
  wsi_object, 
  patch_level = 0,
  custom_downsample = 1,
  patch_size = 256,
  sample_num = 100,
  seed = 1,
  stitch = True,
  mode = 'w',
  verbose = False
):
  file = h5py.File(coords_file_path, 'r')
  dset = file['coords']
  coords = dset[:]

  h5_patch_size = dset.attrs['patch_size']
  h5_patch_level = dset.attrs['patch_level']
  
  if verbose:
    print('in .h5 file: total number of patches: {}'.format(len(coords)))
    print('in .h5 file: patch size: {}x{} patch level: {}'.format(h5_patch_size, h5_patch_size, h5_patch_level))

  if patch_level < 0: patch_level = h5_patch_level
  if patch_size < 0: patch_size = h5_patch_size

  np.random.seed(seed)
  indices = np.random.choice(np.arange(len(coords)), min(len(coords), sample_num), replace=False)
  target_patch_size = np.array([patch_size, patch_size])
  
  if custom_downsample > 1:
    target_patch_size = (np.array([patch_size, patch_size]) / custom_downsample).astype(np.int32)

  canvas = None
  if stitch: 
    canvas = MosaicCanvas(
      patch_size = target_patch_size[0], 
      n = sample_num,
      downscale = 4,
      n_per_row = 10,
      bg_color = (0, 0, 0),
      alpha = -1
    )
  
  for idx in indices:
    coord = coords[idx]
    patch = wsi_object.wsi.read_region(coord, patch_level, tuple([patch_size, patch_size])).convert('RGB')
    
    if custom_downsample > 1: patch = patch.resize(tuple(target_patch_size))
    if stitch: canvas.paste_patch(patch)

    asset_dict = {
      'imgs': np.array(patch)[np.newaxis, ...], 
      'coords': coord
    }
    
    save_hdf5(save_file_path, asset_dict, mode=mode)
    mode='a'

  return canvas, len(coords), len(indices)

[FILE-PATH: utils/wsi_core/wsi_util_classes.py]
[CODE]
import numpy as np
import cv2
from PIL import Image

Image.MAX_IMAGE_PIXELS = 200000000000

class MosaicCanvas(object):
  def __init__(
    self,
    patch_size = 256,
    n = 100,
    downscale = 4,
    n_per_row = 10,
    bg_color = (0, 0, 0),
    alpha = -1
  ):
    self.patch_size = patch_size
    self.downscaled_patch_size = int(np.ceil(patch_size / downscale))
    self.n_rows = int(np.ceil(n / n_per_row))
    self.n_cols = n_per_row
    
    w = self.n_cols * self.downscaled_patch_size
    h = self.n_rows * self.downscaled_patch_size
    
    if alpha < 0: canvas = Image.new(size=(w, h), mode="RGB", color = bg_color)
    else: canvas = Image.new(size=(w, h), mode="RGBA", color = bg_color + (int(255 * alpha),))

    self.canvas = canvas
    self.dimensions = np.array([w, h])
    self.reset_coord()

  def reset_coord(self):
    self.coord = np.array([0, 0])

  def increment_coord(self):
    assert np.all(self.coord<=self.dimensions)
    if self.coord[0] + self.downscaled_patch_size <=self.dimensions[0] - self.downscaled_patch_size:
      self.coord[0]+=self.downscaled_patch_size
    else:
      self.coord[0] = 0 
      self.coord[1]+=self.downscaled_patch_size

  def save(self, save_path, **kwargs):
    self.canvas.save(save_path, **kwargs)

  def paste_patch(self, patch):
    assert patch.size[0] == self.patch_size
    assert patch.size[1] == self.patch_size
    self.canvas.paste(patch.resize(tuple([self.downscaled_patch_size, self.downscaled_patch_size])), tuple(self.coord))
    self.increment_coord()

  def get_painting(self):
    return self.canvas

class ContourCheckingFn(object):
	def __call__(self, pt): 
		raise NotImplementedError

class IsInContourV1(ContourCheckingFn):
	def __init__(self, contour):
		self.cont = contour

	def __call__(self, pt): 
		return 1 if cv2.pointPolygonTest(self.cont, tuple(np.array(pt).astype(float)), False) >= 0 else 0

class IsInContourV2(ContourCheckingFn):
	def __init__(self, contour, patch_size):
		self.cont = contour
		self.patch_size = patch_size

	def __call__(self, pt): 
		pt = np.array((pt[0]+self.patch_size//2, pt[1]+self.patch_size//2)).astype(float)
		return 1 if cv2.pointPolygonTest(self.cont, tuple(np.array(pt).astype(float)), False) >= 0 else 0

# Easy version of 4pt contour checking function - 1 of 4 points need to be in the contour for test to pass
class IsInContourV3Easy(ContourCheckingFn):
	def __init__(self, contour, patch_size, center_shift=0.5):
		self.cont = contour
		self.patch_size = patch_size
		self.shift = int(patch_size//2*center_shift)
  
	def __call__(self, pt): 
		center = (pt[0]+self.patch_size//2, pt[1]+self.patch_size//2)
		if self.shift > 0:
			all_points = [
        (center[0]-self.shift, center[1]-self.shift),
				(center[0]+self.shift, center[1]+self.shift),
				(center[0]+self.shift, center[1]-self.shift),
				(center[0]-self.shift, center[1]+self.shift)
			]
		else:
			all_points = [center]
		
		for points in all_points:
			if cv2.pointPolygonTest(self.cont, tuple(np.array(points).astype(float)), False) >= 0:
				return 1
		return 0

# Hard version of 4pt contour checking function - all 4 points need to be in the contour for test to pass
class IsInContourV3Hard(ContourCheckingFn):
	def __init__(self, contour, patch_size, center_shift=0.5):
		self.cont = contour
		self.patch_size = patch_size
		self.shift = int(patch_size//2*center_shift)
  
	def __call__(self, pt): 
		center = (pt[0]+self.patch_size//2, pt[1]+self.patch_size//2)
		if self.shift > 0:
			all_points = [
        (center[0]-self.shift, center[1]-self.shift),
				(center[0]+self.shift, center[1]+self.shift),
				(center[0]+self.shift, center[1]-self.shift),
				(center[0]-self.shift, center[1]+self.shift)
			]
		else:
			all_points = [center]
		
		for points in all_points:
			if cv2.pointPolygonTest(self.cont, tuple(np.array(points).astype(float)), False) < 0:
				return 0

		return 1





[FILE-PATH: scripts/evaluate_and_gradcam.py]
[CODE]